% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Data Mining},
  pdfauthor={Thiyanga S. Talagala},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Data Mining}
\author{Thiyanga S. Talagala}
\date{2025-10-11}

\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

\bookmarksetup{startatroot}

\chapter{Introduction to Data Mining}\label{introduction-to-data-mining}

\section{What is Data Mining?}\label{what-is-data-mining}

â€¢ Process of discovering interesting patterns of knowledge from huge
amounts of data.

\section{What do we mean by interesting
patterns?}\label{what-do-we-mean-by-interesting-patterns}

â€¢ Interesting patterns: Valid, Novel, Useful, Understandable

Example

â€¢ Retailers collect data about customer purchases at the checkout
counters

â€¢ Customer purchasing patterns: Identify which items are frequently sold
together?

â€¢ Products that are likely to be purchased together.

Why it is useful?

â€¢ Can make a purchase suggestion to their customers

â€¢ Gives an idea that how we can arrange items in a store to as a
strategy for boosting sales.

\section{Characteristics of Big Data: 5 V's of Big
Data}\label{characteristics-of-big-data-5-vs-of-big-data}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Volume: size
\item
  Velocity: how quickly data is generated?
\item
  Variety: diversity
\item
  Veracity: quality of data
\item
  Value: how useful?
\end{enumerate}

\section{What motivates the development of data mining
field?}\label{what-motivates-the-development-of-data-mining-field}

â€¢ Scalability

â€¢ High dimensionality

â€¢ Heterogeneous and complex data

â€¢ Data ownership and distribution

\section{Data Mining Tasks}\label{data-mining-tasks}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Predictive tasks: Predict the value of a particular attribute based on
  the values of other attributes
\item
  Descriptive tasks: Find human-interpretable patterns that describe
  data
\end{enumerate}

\section{Data Quality}\label{data-quality}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Range: How narrow or wide of the scope of these data?
\item
  Relevancy: Is the data relevant to the problem?
\item
  Recency: How recent the data is generated?
\item
  Robustness: Signal to noise ratio
\item
  Reliability: How accurate?
\end{enumerate}

\section{Applications}\label{applications}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Web mining: recommendation systems
\item
  Screening images: Early warning of ecological disasters
\item
  Marketing and sales
\item
  Diagnosis
\item
  Load forecasting
\item
  Decision involving judgement
\end{enumerate}

Many more\ldots{}

\bookmarksetup{startatroot}

\chapter{What is Statistical
Learing?}\label{what-is-statistical-learing}

In-class explanations

\bookmarksetup{startatroot}

\chapter{Decision Trees}\label{decision-trees}

\section{Motivational Example
Dataset}\label{motivational-example-dataset}

Features: Sepal Length, Sepal Width

Outcome: Species: setosa/versicolor

\subsection{Predictor space}\label{predictor-space}

\includegraphics[width=\linewidth,height=0.7\textheight,keepaspectratio]{ch3_files/figure-pdf/unnamed-chunk-1-1.pdf}

\subsection{Decision Tree}\label{decision-tree}

\pandocbounded{\includegraphics[keepaspectratio]{ch3_files/figure-pdf/unnamed-chunk-2-1.pdf}}

\subsection{Partition space}\label{partition-space}

\pandocbounded{\includegraphics[keepaspectratio]{ch3_files/figure-pdf/unnamed-chunk-3-1.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{ch3_files/figure-pdf/unnamed-chunk-4-1.pdf}}

\(R1 = \{X|Sepal.Width >=3, Sepal.Length <5.5
\}\)

\section{Parts of a decision tree}\label{parts-of-a-decision-tree}

\begin{itemize}
\item
  Root node
\item
  Decision node
\item
  Terminal node/ Leaf node (gives outputs/class assignments)
\item
  Subtree
\end{itemize}

\section{Introduction}\label{introduction}

What happens in the model training phase?

\begin{itemize}
\item
  Stratify or segment the predictor space into a number of
  non-overlapping regions.
\item
  Set of splitting rules are used to segment the predictor space.
\item
  Decision tree consists of a series of splitting rules.
\end{itemize}

How to make predictions?

\begin{itemize}
\item
  Mean or mode response value for the training observations in the
  region which the observation we want to predict belong to.
\item
  We make the same prediction, for the training observations in the
  \(j^{th}\) region \(R_j\).
\end{itemize}

\section{Decision Trees: Regression - Finding the best split and best
splitting
variable}\label{decision-trees-regression---finding-the-best-split-and-best-splitting-variable}

The goal is to find \(R_1, R_2, R_3...R_J\), \(J\) distinct and
non-overlapping regions that minimize the RSS given by

\[\sum_{j=1}^{J}\sum_{i \in R_j}(y_i - \hat{y}_{R_j})^2.\]
\(\hat{y}_{R_j}\) - mean response for the training observations within
the \(j^{th}\) region.

In theory, to build the best possible decision tree, we could try every
possible way of splitting the data at every step and choose the one that
gives the best results. However, this is computationally infeasible,
especially when the dataset is large or when there are many predictor
variables --- the number of possible splits grows exponentially.

Therefore, instead of trying all possible combinations at once, we use a
\textbf{recursive partitioning approach}. This means:

Start with the entire dataset as one group (the root node).

Find the single best split --- the one that most effectively separates
the data based on the target variable.

Divide the data into two or more subgroups (child nodes) based on that
split.

Repeat the process (recursively) within each subgroup: again find the
best split, divide the data, and continue until a stopping rule is met
(for example, minimum node size or maximum tree depth).

This recursive process allows the algorithm to build the tree step by
step, finding locally optimal splits that approximate the best possible
tree without having to evaluate every possible combination.

\section{Recursive Partitioning for Regression
Trees}\label{recursive-partitioning-for-regression-trees}

A regression tree predicts a continuous outcome (for example, house
price, temperature, or pH level). The goal is to split the data into
smaller and smaller groups (nodes) that are as homogeneous as possible
with respect to the response variable.

In-class notation

\section{Recursive Partitioning for Regression
Trees}\label{recursive-partitioning-for-regression-trees-1}

A \textbf{regression tree} is used when the response variable is
\emph{continuous} (e.g., house price, temperature, or pH level).

The goal is to split the data into smaller and smaller groups (nodes)
that are as \textbf{homogeneous as possible} with respect to the
response.

\subsection{Step 1: Start with All
Predictors}\label{step-1-start-with-all-predictors}

At the beginning, the algorithm considers \textbf{all predictor
variables}:

\[X_1, X_2, X_3, \dots, X_p\]

For each predictor, it looks for the \textbf{best split point} that
divides the data into two groups such that the prediction error (usually
the \textbf{sum of squared errors, SSE}) is minimized.

\subsection{Step 2: Consider All Possible
Values}\label{step-2-consider-all-possible-values}

\subsubsection{Continuous predictors}\label{continuous-predictors}

If a predictor \(X_j\) is \textbf{continuous},\\
then ``all possible values'' refers to \textbf{all unique values} (or
\textbf{midpoints between consecutive sorted values}) that can be used
to split the data.

Example:

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Observation & \(X_j\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 2 \\
2 & 3 \\
3 & 5 \\
4 & 7 \\
\end{longtable}

Possible split points (midpoints): - \(X_j < 2.5\) - \(X_j < 4\) -
\(X_j < 6\)

Each of these potential splits is tested to see how well it reduces the
SSE of the response variable.

\subsubsection{Categorical predictors}\label{categorical-predictors}

If \(X_j\) is \textbf{categorical},\\
``all possible values'' means all possible \textbf{groupings (subsets)}
of the categories.

Example:\\
If the predictor \texttt{Species} = \{A, B, C\}, possible splits are:

\begin{itemize}
\item
  \{A\} vs \{B, C\}
\item
  \{B\} vs \{A, C\}
\item
  \{C\} vs \{A, B\}
\end{itemize}

Each grouping is evaluated based on how much it reduces the variability
in the response.

\subsection{Step 3: Choose the Best
Split}\label{step-3-choose-the-best-split}

For every predictor and every possible split value, compute:

\[
\text{SSE}_{\text{split}} = \text{SSE}_{\text{left node}} + \text{SSE}_{\text{right node}}
\]

The \textbf{split that minimizes} this total SSE is chosen as the best
split for that node.

\subsection{Step 4: Recursive
Partitioning}\label{step-4-recursive-partitioning}

After splitting the data into two nodes, the same process is applied
\textbf{recursively} within each node:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Consider all predictors again.\\
\item
  For each predictor, test all possible values.\\
\item
  Find the best split within that node.\\
\item
  Continue until a stopping rule is met (e.g., minimum node size or no
  significant improvement).
\end{enumerate}

\subsection{Summary}\label{summary}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Explanation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{All predictors} & Every variable \(X_1, X_2, \dots, X_p\) is
considered at each split. \\
\textbf{All possible values} & Every unique value (or midpoint between
values) is tested as a potential split. \\
\textbf{Recursive partitioning} & The process of repeatedly splitting
the data into smaller homogeneous groups until a stopping rule is
met. \\
\end{longtable}

\begin{quote}
\textbf{In summary:}\\
Finding every possible combination of splits is computationally
infeasible.\\
Recursive partitioning provides a practical, step-by-step method that
finds locally optimal splits efficiently.
\end{quote}

\section{Depth of the decision tree}\label{depth-of-the-decision-tree}

\pandocbounded{\includegraphics[keepaspectratio]{ch3_files/figure-pdf/unnamed-chunk-5-1.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{ch3_files/figure-pdf/unnamed-chunk-5-2.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{ch3_files/figure-pdf/unnamed-chunk-6-1.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{ch3_files/figure-pdf/unnamed-chunk-6-2.pdf}}

\section{Pruning Regression Trees}\label{pruning-regression-trees}

Once a regression tree is grown, it often becomes \textbf{too complex}
--- it may fit the training data very well but perform poorly on new,
unseen data.\\
This problem is known as \textbf{overfitting}.

\textbf{Pruning} is the process of \textbf{reducing the size} of a fully
grown tree to improve its ability to generalize.

\subsection{Why Prune?}\label{why-prune}

\begin{itemize}
\tightlist
\item
  A large tree captures \textbf{noise} as if it were structure.\\
\item
  It has \textbf{low bias} but \textbf{high variance}.\\
\item
  Pruning helps to find a \textbf{balance between model complexity and
  prediction accuracy}.
\end{itemize}

\subsection{Types of Pruning}\label{types-of-pruning}

There are two main approaches:

\subsubsection{(a) Pre-pruning (Early
stopping)}\label{a-pre-pruning-early-stopping}

Stop the tree growth \textbf{before} it becomes too large.

Common stopping rules:

\begin{itemize}
\item
  Minimum number of observations in a node
\item
  Maximum tree depth
\item
  Minimum decrease in SSE required for a split
\end{itemize}

\subsubsection{(b) Post-pruning (Cost Complexity
Pruning)}\label{b-post-pruning-cost-complexity-pruning}

Grow a \textbf{large tree first}, then prune it \textbf{backward} by
removing branches that contribute little to predictive accuracy.

\subsection{Cost Complexity Pruning (a.k.a. Weakest Link
Pruning)}\label{cost-complexity-pruning-a.k.a.-weakest-link-pruning}

The idea is to penalize tree size using a complexity parameter
(\(\lambda\)).

For any subtree ( T ):

\[
C(T) = \text{Error}(T) + \lambda L(T)
\]

where:

\begin{itemize}
\tightlist
\item
  \text{Error}(T): measure of fit (e.g., sum of squared errors)\\
\item
  \(L(T)\): number of leaf nodes in the tree (measure of complexity)\\
\item
  \(\lambda\): penalty factor controlling the trade-off between
  complexity and predictive power
\end{itemize}

\subsection{Interpretation of
Parameters}\label{interpretation-of-parameters}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(\lambda = 0\) & Fully grown decision tree (no penalty for
complexity). \\
\(\lambda = \infty\) & Root node only (maximum penalty, no splits). \\
\(0 < \lambda < \infty\) & Balances predictive power and complexity. \\
\end{longtable}

\subsection{Total Cost Components}\label{total-cost-components}

\[
\text{Total Cost} = \text{Measure of Fit} + \text{Measure of Complexity}
\]

\begin{itemize}
\tightlist
\item
  \textbf{Measure of Fit:} Error (e.g., SSE)\\
\item
  \textbf{Measure of Complexity:} Number of leaf nodes \(L(T)\)
\end{itemize}

So, \[
C(T) = \text{Error}(T) + \lambda L(T)
\]

This is sometimes written as:

\[
R_\lambda(T) = R(T) + \lambda |T|
\]

Both expressions represent the same concept --- a \textbf{trade-off
between model fit and simplicity}.

\section{Example R code for Pre-pruning and
Post-pruning}\label{example-r-code-for-pre-pruning-and-post-pruning}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary packages}
\FunctionTok{library}\NormalTok{(rpart)}
\FunctionTok{library}\NormalTok{(rpart.plot)}

\CommentTok{\# Use built{-}in dataset}
\FunctionTok{data}\NormalTok{(mtcars)}

\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# ðŸŒ± 1. Pre{-}pruning (Early stopping)}
\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# Control parameters limit tree growth}
\NormalTok{prepruned\_tree }\OtherTok{\textless{}{-}} \FunctionTok{rpart}\NormalTok{(}
\NormalTok{  mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
  \AttributeTok{data =}\NormalTok{ mtcars,}
  \AttributeTok{method =} \StringTok{"anova"}\NormalTok{,}
  \AttributeTok{control =} \FunctionTok{rpart.control}\NormalTok{(}
    \AttributeTok{minsplit =} \DecValTok{10}\NormalTok{,   }\CommentTok{\# minimum observations required to attempt a split}
    \AttributeTok{cp =} \FloatTok{0.02}\NormalTok{,       }\CommentTok{\# minimum improvement in SSE required for a split}
    \AttributeTok{maxdepth =} \DecValTok{3}     \CommentTok{\# maximum depth of the tree}
\NormalTok{  )}
\NormalTok{)}

\CommentTok{\# Visualize the pre{-}pruned tree}
\FunctionTok{rpart.plot}\NormalTok{(prepruned\_tree, }\AttributeTok{main =} \StringTok{"Pre{-}pruned Regression Tree"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ch3_files/figure-pdf/unnamed-chunk-7-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Print summary}
\FunctionTok{print}\NormalTok{(prepruned\_tree)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
n= 32 

node), split, n, deviance, yval
      * denotes terminal node

 1) root 32 1126.047000 20.09062  
   2) wt>=2.26 26  346.566500 17.78846  
     4) cyl>=7 14   85.200000 15.10000  
       8) disp>=420 3   12.326670 11.83333 *
       9) disp< 420 11   32.129090 15.99091 *
     5) cyl< 7 12   42.122500 20.92500  
      10) wt>=3.3275 3    1.086667 18.36667 *
      11) wt< 3.3275 9   14.855560 21.77778 *
   3) wt< 2.26 6   44.553330 30.06667 *
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(prepruned\_tree)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call:
rpart(formula = mpg ~ ., data = mtcars, method = "anova", control = rpart.control(minsplit = 10, 
    cp = 0.02, maxdepth = 3))
  n= 32 

          CP nsplit rel error    xerror       xstd
1 0.65266121      0 1.0000000 1.0532743 0.25701075
2 0.19470235      1 0.3473388 0.6002473 0.11830747
3 0.03618342      2 0.1526364 0.3143704 0.06382314
4 0.02324972      3 0.1164530 0.2865804 0.05941420
5 0.02000000      4 0.0932033 0.2988887 0.06843228

Variable importance
  wt disp   hp drat  cyl qsec   vs 
  27   25   19   11    9    5    5 

Node number 1: 32 observations,    complexity param=0.6526612
  mean=20.09062, MSE=35.18897 
  left son=2 (26 obs) right son=3 (6 obs)
  Primary splits:
      wt   < 2.26   to the right, improve=0.6526612, (0 missing)
      cyl  < 5      to the right, improve=0.6431252, (0 missing)
      disp < 163.8  to the right, improve=0.6130502, (0 missing)
      hp   < 118    to the right, improve=0.6010712, (0 missing)
      vs   < 0.5    to the left,  improve=0.4409477, (0 missing)
  Surrogate splits:
      disp < 101.55 to the right, agree=0.969, adj=0.833, (0 split)
      hp   < 92     to the right, agree=0.938, adj=0.667, (0 split)
      drat < 4      to the left,  agree=0.906, adj=0.500, (0 split)
      cyl  < 5      to the right, agree=0.844, adj=0.167, (0 split)

Node number 2: 26 observations,    complexity param=0.1947024
  mean=17.78846, MSE=13.32948 
  left son=4 (14 obs) right son=5 (12 obs)
  Primary splits:
      cyl  < 7      to the right, improve=0.6326174, (0 missing)
      disp < 266.9  to the right, improve=0.6326174, (0 missing)
      hp   < 136.5  to the right, improve=0.5803554, (0 missing)
      wt   < 3.325  to the right, improve=0.5393370, (0 missing)
      qsec < 18.15  to the left,  improve=0.4210605, (0 missing)
  Surrogate splits:
      disp < 266.9  to the right, agree=1.000, adj=1.000, (0 split)
      hp   < 136.5  to the right, agree=0.962, adj=0.917, (0 split)
      wt   < 3.49   to the right, agree=0.885, adj=0.750, (0 split)
      qsec < 18.15  to the left,  agree=0.885, adj=0.750, (0 split)
      vs   < 0.5    to the left,  agree=0.885, adj=0.750, (0 split)

Node number 3: 6 observations
  mean=30.06667, MSE=7.425556 

Node number 4: 14 observations,    complexity param=0.03618342
  mean=15.1, MSE=6.085714 
  left son=8 (3 obs) right son=9 (11 obs)
  Primary splits:
      disp < 420    to the right, improve=0.4782188, (0 missing)
      wt   < 4.66   to the right, improve=0.4782188, (0 missing)
      hp   < 192.5  to the right, improve=0.4669349, (0 missing)
      carb < 3.5    to the right, improve=0.4669349, (0 missing)
      qsec < 17.71  to the right, improve=0.4306658, (0 missing)
  Surrogate splits:
      wt   < 4.66   to the right, agree=1.000, adj=1.000, (0 split)
      drat < 3.035  to the left,  agree=0.857, adj=0.333, (0 split)
      qsec < 17.41  to the right, agree=0.857, adj=0.333, (0 split)

Node number 5: 12 observations,    complexity param=0.02324972
  mean=20.925, MSE=3.510208 
  left son=10 (3 obs) right son=11 (9 obs)
  Primary splits:
      wt   < 3.3275 to the right, improve=0.6215272, (0 missing)
      cyl  < 5      to the right, improve=0.5573591, (0 missing)
      hp   < 96     to the right, improve=0.5507811, (0 missing)
      disp < 163.8  to the right, improve=0.4615111, (0 missing)
      carb < 3      to the right, improve=0.2857431, (0 missing)
  Surrogate splits:
      disp < 163.8  to the right, agree=0.917, adj=0.667, (0 split)
      hp   < 116.5  to the right, agree=0.833, adj=0.333, (0 split)

Node number 8: 3 observations
  mean=11.83333, MSE=4.108889 

Node number 9: 11 observations
  mean=15.99091, MSE=2.920826 

Node number 10: 3 observations
  mean=18.36667, MSE=0.3622222 

Node number 11: 9 observations
  mean=21.77778, MSE=1.650617 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# ðŸŒ³ 2. Post{-}pruning (Cost{-}complexity pruning)}
\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# Step 1: Grow a large tree first}
\NormalTok{full\_tree }\OtherTok{\textless{}{-}} \FunctionTok{rpart}\NormalTok{(}
\NormalTok{  mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
  \AttributeTok{data =}\NormalTok{ mtcars,}
  \AttributeTok{method =} \StringTok{"anova"}\NormalTok{,}
  \AttributeTok{control =} \FunctionTok{rpart.control}\NormalTok{(}\AttributeTok{cp =} \FloatTok{0.0001}\NormalTok{)  }\CommentTok{\# allow the tree to grow large}
\NormalTok{)}

\CommentTok{\# Step 2: Display complexity parameter (CP) table}
\FunctionTok{printcp}\NormalTok{(full\_tree)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Regression tree:
rpart(formula = mpg ~ ., data = mtcars, method = "anova", control = rpart.control(cp = 1e-04))

Variables actually used in tree construction:
[1] cyl hp 

Root node error: 1126/32 = 35.189

n= 32 

        CP nsplit rel error  xerror    xstd
1 0.643125      0   1.00000 1.08252 0.26021
2 0.097484      1   0.35687 0.80770 0.18280
3 0.000100      2   0.25939 0.67304 0.15252
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Step 3: Plot cross{-}validation results}
\FunctionTok{plotcp}\NormalTok{(full\_tree, }\AttributeTok{main =} \StringTok{"Cost{-}Complexity Pruning Plot"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ch3_files/figure-pdf/unnamed-chunk-7-2.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Step 4: Select the cp value that minimizes cross{-}validation error}
\NormalTok{optimal\_cp }\OtherTok{\textless{}{-}}\NormalTok{ full\_tree}\SpecialCharTok{$}\NormalTok{cptable[}\FunctionTok{which.min}\NormalTok{(full\_tree}\SpecialCharTok{$}\NormalTok{cptable[,}\StringTok{"xerror"}\NormalTok{]), }\StringTok{"CP"}\NormalTok{]}

\CommentTok{\# Step 5: Prune the tree at the optimal cp value}
\NormalTok{pruned\_tree }\OtherTok{\textless{}{-}} \FunctionTok{prune}\NormalTok{(full\_tree, }\AttributeTok{cp =}\NormalTok{ optimal\_cp)}

\CommentTok{\# Step 6: Visualize the pruned tree}
\FunctionTok{rpart.plot}\NormalTok{(pruned\_tree, }\AttributeTok{main =} \StringTok{"Post{-}pruned Regression Tree"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ch3_files/figure-pdf/unnamed-chunk-7-3.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Step 7: Summarize pruned model}
\FunctionTok{summary}\NormalTok{(pruned\_tree)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call:
rpart(formula = mpg ~ ., data = mtcars, method = "anova", control = rpart.control(cp = 1e-04))
  n= 32 

          CP nsplit rel error    xerror      xstd
1 0.64312523      0 1.0000000 1.0825169 0.2602079
2 0.09748407      1 0.3568748 0.8076995 0.1828019
3 0.00010000      2 0.2593907 0.6730430 0.1525172

Variable importance
 cyl disp   hp   wt qsec   vs carb 
  20   20   19   16   12   11    1 

Node number 1: 32 observations,    complexity param=0.6431252
  mean=20.09062, MSE=35.18897 
  left son=2 (21 obs) right son=3 (11 obs)
  Primary splits:
      cyl  < 5      to the right, improve=0.6431252, (0 missing)
      wt   < 2.3925 to the right, improve=0.6356630, (0 missing)
      disp < 163.8  to the right, improve=0.6130502, (0 missing)
      hp   < 118    to the right, improve=0.6010712, (0 missing)
      vs   < 0.5    to the left,  improve=0.4409477, (0 missing)
  Surrogate splits:
      disp < 142.9  to the right, agree=0.969, adj=0.909, (0 split)
      hp   < 101    to the right, agree=0.938, adj=0.818, (0 split)
      wt   < 2.5425 to the right, agree=0.906, adj=0.727, (0 split)
      qsec < 18.41  to the left,  agree=0.844, adj=0.545, (0 split)
      vs   < 0.5    to the left,  agree=0.844, adj=0.545, (0 split)

Node number 2: 21 observations,    complexity param=0.09748407
  mean=16.64762, MSE=9.451066 
  left son=4 (7 obs) right son=5 (14 obs)
  Primary splits:
      hp   < 192.5  to the right, improve=0.5530828, (0 missing)
      cyl  < 7      to the right, improve=0.5068475, (0 missing)
      disp < 266.9  to the right, improve=0.5068475, (0 missing)
      wt   < 3.49   to the right, improve=0.4414890, (0 missing)
      drat < 3.075  to the left,  improve=0.1890739, (0 missing)
  Surrogate splits:
      disp < 334    to the right, agree=0.857, adj=0.571, (0 split)
      wt   < 4.66   to the right, agree=0.810, adj=0.429, (0 split)
      qsec < 15.455 to the left,  agree=0.810, adj=0.429, (0 split)
      carb < 3.5    to the right, agree=0.762, adj=0.286, (0 split)
      gear < 4.5    to the right, agree=0.714, adj=0.143, (0 split)

Node number 3: 11 observations
  mean=26.66364, MSE=18.48959 

Node number 4: 7 observations
  mean=13.41429, MSE=4.118367 

Node number 5: 14 observations
  mean=18.26429, MSE=4.276582 
\end{verbatim}

\section{Classification Trees: Best Split, Entropy, and Gini
Coefficients}\label{classification-trees-best-split-entropy-and-gini-coefficients}

Classification trees are used when the response variable is
\textbf{categorical}.\\
At each node, the algorithm tries to find the \textbf{best split} ---
the one that produces the most \textbf{homogeneous} (pure) child nodes.

\subsection{The Idea of ``Best Split''}\label{the-idea-of-best-split}

At any node in the tree:

\begin{itemize}
\tightlist
\item
  The data are divided into two (or more) groups based on a predictor.
\item
  The \textbf{best split} is the one that makes each resulting group as
  \textbf{pure} as possible with respect to the class labels.
\end{itemize}

To measure \textbf{purity}, we use \textbf{impurity measures} such as:

\begin{itemize}
\item
  \textbf{Entropy}
\item
  \textbf{Gini index}
\item
  (Sometimes) \textbf{Misclassification error}
\end{itemize}

\subsection{Entropy}\label{entropy}

Entropy measures the \textbf{disorder} or \textbf{uncertainty} in a
node.

If there are ( K ) classes and ( p\_k ) is the proportion of
observations belonging to class ( k ), then:

\[
\text{Entropy} = - \sum_{k=1}^{K} p_k \log_2(p_k)
\]

\subsubsection{Properties:}\label{properties}

\begin{itemize}
\tightlist
\item
  Entropy = 0 â†’ Node is perfectly pure (all observations belong to one
  class).
\item
  Entropy is maximum when all classes are equally likely.
\end{itemize}

\subsubsection{Example:}\label{example}

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Class & Count & \(p_k\) & \(-p_k \log_2(p_k)\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
A & 8 & 0.8 & 0.257 \\
B & 2 & 0.2 & 0.464 \\
\end{longtable}

\[
\text{Entropy} = 0.257 + 0.464 = 0.721
\]

\subsection{Gini Index}\label{gini-index}

The \textbf{Gini coefficient} (or \textbf{Gini impurity}) is another
measure of node impurity.

\[
\text{Gini} = 1 - \sum_{k=1}^{K} p_k^2
\]

\subsubsection{Properties:}\label{properties-1}

\begin{itemize}
\item
  Gini = 0 â†’ Perfectly pure node
\item
  Gini is smaller when the node is more homogeneous
\end{itemize}

\subsubsection{Example:}\label{example-1}

Using the same proportions as above ( \(p_1 = 0.8, p_2 = 0.2\) ):

\[
\text{Gini} = 1 - (0.8^2 + 0.2^2) = 1 - (0.64 + 0.04) = 0.32
\]

\subsection{Misclassification Error (less
common)}\label{misclassification-error-less-common}

A simpler impurity measure sometimes used:

\[
\text{Error} = 1 - \max(p_k)
\]

For the same node ( \(\max(p_k) = 0.8\) ):

\[
\text{Error} = 1 - 0.8 = 0.2
\]

\subsection{Choosing the Best Split}\label{choosing-the-best-split}

For each possible split:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Compute the impurity (Entropy or Gini) \textbf{before} splitting ---
  call this \(I_{\text{parent}}\).
\item
  Compute the impurity of the \textbf{child nodes} (weighted by their
  sizes):
\end{enumerate}

\[
I_{\text{split}} = \frac{n_L}{n} I_L + \frac{n_R}{n} I_R
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Compute the \textbf{information gain} (reduction in impurity):
\end{enumerate}

\[
\text{Gain} = I_{\text{parent}} - I_{\text{split}}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  The \textbf{best split} is the one that maximizes this gain (i.e.,
  gives the largest reduction in impurity).
\end{enumerate}

\subsection{Comparing Entropy and
Gini}\label{comparing-entropy-and-gini}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Property
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Entropy
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Gini
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Range & 0 to 1 & 0 to 0.5 \\
Shape & Logarithmic & Quadratic \\
Interpretation & Information theory measure & Probability of
misclassification \\
Behavior & Slightly more sensitive to rare classes & Computationally
simpler \\
Commonly used in & C4.5, ID3 algorithms & CART algorithm \\
\end{longtable}

Both criteria often lead to \textbf{similar splits} in practice.

\bookmarksetup{startatroot}

\chapter{Random Forests}\label{random-forests}

\section{Decision trees - Limitation}\label{decision-trees---limitation}

To capture a complex decision boundary we need to use a deep tree

In-class explanation

\section{Bias-Variance Trade off}\label{bias-variance-trade-off}

\begin{itemize}
\tightlist
\item
  A deep decision tree has low bias and high variance.
\end{itemize}

\pandocbounded{\includegraphics[keepaspectratio]{bv.jpeg}}

\section{Bagging (Bootstrap
Aggregation)}\label{bagging-bootstrap-aggregation}

\begin{itemize}
\item
  Technique for reducing the variance of an estimated predicted function
\item
  Works well for high-variance, low-bias procedures, such as trees
\end{itemize}

\section{Ensemble Methods}\label{ensemble-methods}

\begin{itemize}
\item
  Combines several base models
\item
  \textbf{Bagg}ing (\textbf{B}ootstrap \textbf{Agg}regation) is an
  ensemble method
\end{itemize}

\section{Ensemble Methods}\label{ensemble-methods-1}

``Ensemble learning gives credence to the idea of the ``wisdom of
crowds,'' which suggests that the decision-making of a larger group of
people is typically better than that of an individual expert.''

Source: \url{https://www.ibm.com/cloud/learn/boosting}

\section{Bootstrap}\label{bootstrap}

\begin{itemize}
\tightlist
\item
  Generate multiple samples of training data, via bootstrapping
\end{itemize}

Example

Training data: \(\{(y_1, x_1), (y_2, x_2), (y_3, x_3), (y_4, x_4)\}\)

Three samples generated from bootstrapping

Sample 1 = \(\{(y_1, x_1), (y_2, x_2), (y_3, x_3), (y_4, x_4)\}\)

Sample 2 = \(\{(y_1, x_1), (y_1, x_1), (y_1, x_1), (y_4, x_4)\}\)

Sample 3 = \(\{(y_1, x_1), (y_2, x_2), (y_1, x_1), (y_4, x_4)\}\)

\section{Aggregation}\label{aggregation}

\begin{itemize}
\item
  Train a decision tree on each bootstrap sample of data without
  pruning.
\item
  Aggregate prediction using either voting or averaging
\end{itemize}

\section{Bagging - in class diagram}\label{bagging---in-class-diagram}

\section{Bagging}\label{bagging}

\textbf{Pros}

\begin{itemize}
\item
  Ease of implementation
\item
  Reduction of variance
\end{itemize}

\textbf{Cons}

\begin{itemize}
\item
  Loss of interpretability
\item
  Computationally expensive
\end{itemize}

\section{Bagging}\label{bagging-1}

\begin{itemize}
\item
  Bootstrapped subsamples are created
\item
  A Decision Tree is formed on each bootstrapped sample.
\item
  The results of each tree are aggregated
\end{itemize}

\section{Random Forests: Improving on
Bagging}\label{random-forests-improving-on-bagging}

\begin{itemize}
\item
  The ensembles of trees in Bagging tend to be highly correlated.
\item
  All of the bagged trees will look quite similar to each other. Hence,
  the predictions from the bagged trees will be highly correlated.
\end{itemize}

\section{Random Forests}\label{random-forests-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Bootstrap samples
\item
  At each split, randomly select a set of predictors from the full set
  of predictors
\item
  From the selected predictors we select the optimal predictor and the
  optimal corresponding threshold for the split.
\item
  Grow multiple trees and aggregate
\end{enumerate}

\section{Random Forests - Hyper
parameters}\label{random-forests---hyper-parameters}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Number of variables randomly sampled as candidates at each split
\item
  Number of trees to grow
\item
  Minimum size of terminal nodes. Setting this number larger causes
  smaller trees to be grown (and thus take less time).
\end{enumerate}

Note: In theory, each tree in the random forest is full (not pruned),
but in practice this can be computationally expensive,thus, imposing a
minimum node size is not unusual.

\section{Random Forests}\label{random-forests-2}

\begin{itemize}
\item
  Bagging ensemble method
\item
  Gives final prediction by aggregating the predictions of bootstrapped
  decision tree samples.
\item
  Trees in a random forest are independent of each other.
\end{itemize}

\section{Random Forests}\label{random-forests-3}

\textbf{Pros}

\begin{itemize}
\tightlist
\item
  Accuracy
\end{itemize}

\textbf{Cons}

\begin{itemize}
\item
  Speed
\item
  Interpretability
\item
  Overfitting
\end{itemize}

\section{Out-of-bag error}\label{out-of-bag-error}

With ensemble methods, we get a new metric for assessing the predictive
performance of the model, the out-of-bag error

\section{Random Forests}\label{random-forests-4}

\pandocbounded{\includegraphics[keepaspectratio]{rf1.png}}

\section{Random Forests}\label{random-forests-5}

\pandocbounded{\includegraphics[keepaspectratio]{rf2.png}}

\section{Out-of-Bag (OOB) Samples}\label{out-of-bag-oob-samples}

\pandocbounded{\includegraphics[keepaspectratio]{rf3.png}}

\section{Out-of-Bag (OOB) Samples}\label{out-of-bag-oob-samples-1}

\pandocbounded{\includegraphics[keepaspectratio]{rf4.png}}

\section{Predictions based on OOB
observations}\label{predictions-based-on-oob-observations}

\pandocbounded{\includegraphics[keepaspectratio]{rf5.png}}

\section{Predictions based on OOB
observations}\label{predictions-based-on-oob-observations-1}

Figure 1

\pandocbounded{\includegraphics[keepaspectratio]{rf6.png}}

Figure 2

\pandocbounded{\includegraphics[keepaspectratio]{rf7.png}}

Figure 3

\pandocbounded{\includegraphics[keepaspectratio]{rf8.png}}

Figure 4

\pandocbounded{\includegraphics[keepaspectratio]{rf9.png}}

Figure 5

\pandocbounded{\includegraphics[keepaspectratio]{rf10.png}}

Figure 6

\pandocbounded{\includegraphics[keepaspectratio]{rf11.png}}

Figure 7

\pandocbounded{\includegraphics[keepaspectratio]{rf12.png}}

Figure 8

\pandocbounded{\includegraphics[keepaspectratio]{rf13.png}}

\section{Variable Importance in Random
Forest}\label{variable-importance-in-random-forest}

\textbf{contribution to predictive accuracy}

\begin{itemize}
\item
  Permutation-based variable importance
\item
  Mean decrease in Gini coefficient
\end{itemize}

\section{Permutation-based variable
importance}\label{permutation-based-variable-importance}

\begin{itemize}
\item
  the OOB samples are passed down the tree, and the prediction accuracy
  is recorded
\item
  the values for the \(j^{th}\) variable are randomly permuted in the
  OOB samples, and the accuracy is again computed.
\item
  the decrease in accuracy as a result of this permuting is averaged
  over all trees, and is used as a measure of the importance of variable
  \(j\) in the random forests
\end{itemize}

\section{Mean decrease in Gini
coefficient}\label{mean-decrease-in-gini-coefficient}

\begin{itemize}
\item
  Measure of how each variable contributes to the homogeneity of the
  nodes and leaves in the resulting random forest
\item
  The higher the value of mean decrease accuracy or mean decrease Gini
  score, the higher the importance of the variable in the model
\end{itemize}

\section{Boosting}\label{boosting}

\begin{itemize}
\item
  Bagging and boosting are two main types of ensemble learning methods.
\item
  The main difference between bagging and boosting is the way in which
  they are trained.
\item
  In bagging, weak learners (decision trees) are trained in parallel,
  but in boosting, they learn sequentially.
\end{itemize}

\section{Boosting}\label{boosting-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Fit a single tree
\item
  Draw a sample that gives higher selection probabilities to
  misclassified records
\item
  Fit a tree to the new sample
\item
  Repeat Steps 2 and 3 multiple times
\item
  Use weighted voting to classify records, with heavier weights for
  later trees
\end{enumerate}

\section{Boosting}\label{boosting-2}

\begin{itemize}
\item
  Iterative process.
\item
  Each tree is dependent on the previous one. Hence, it is hard to
  parallelize the training process of boosting algorithms.
\item
  The training time will be higher. This is the main drawback of
  boosting algorithms.
\end{itemize}

\section{Boosting Algorithms}\label{boosting-algorithms}

\begin{itemize}
\item
  Adaptive boosting or AdaBoost
\item
  Gradient boosting
\item
  Extreme gradient boosting or XGBoost
\end{itemize}

\bookmarksetup{startatroot}

\chapter{Measures of Performance}\label{measures-of-performance}

Task: Build a tree-based model to predict species

\section{Load libraries}\label{load-libraries}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(palmerpenguins)}
\FunctionTok{library}\NormalTok{(rsample)       }\CommentTok{\# for initial\_split}
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(caret)}
\FunctionTok{library}\NormalTok{(randomForest)}
\FunctionTok{library}\NormalTok{(ada)}
\FunctionTok{library}\NormalTok{(gbm)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(yardstick) }\CommentTok{\#roc\_curve}
\end{Highlighting}
\end{Shaded}

\section{Clean data}\label{clean-data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins\_clean }\OtherTok{\textless{}{-}} \FunctionTok{na.omit}\NormalTok{(penguins)}
\NormalTok{penguins\_clean}\SpecialCharTok{$}\NormalTok{species }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(penguins\_clean}\SpecialCharTok{$}\NormalTok{species)}
\end{Highlighting}
\end{Shaded}

\section{Stratified train-test split}\label{stratified-train-test-split}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{split }\OtherTok{\textless{}{-}} \FunctionTok{initial\_split}\NormalTok{(penguins\_clean, }\AttributeTok{prop =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{strata =} \StringTok{"species"}\NormalTok{)}
\NormalTok{train\_data }\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(split)}
\FunctionTok{table}\NormalTok{(train\_data}\SpecialCharTok{$}\NormalTok{species)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

   Adelie Chinstrap    Gentoo 
      102        47        83 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test\_data  }\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(split)}
\FunctionTok{table}\NormalTok{(test\_data}\SpecialCharTok{$}\NormalTok{species)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

   Adelie Chinstrap    Gentoo 
       44        21        36 
\end{verbatim}

\section{Variables}\label{variables}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predictors }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"bill\_length\_mm"}\NormalTok{, }\StringTok{"bill\_depth\_mm"}\NormalTok{, }\StringTok{"flipper\_length\_mm"}\NormalTok{, }\StringTok{"body\_mass\_g"}\NormalTok{, }\StringTok{"sex"}\NormalTok{)}
\NormalTok{response }\OtherTok{\textless{}{-}} \StringTok{"species"}
\end{Highlighting}
\end{Shaded}

\section{Random Forest Model}\label{random-forest-model}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{rf\_model }\OtherTok{\textless{}{-}} \FunctionTok{randomForest}\NormalTok{(}
  \FunctionTok{as.formula}\NormalTok{(}\FunctionTok{paste}\NormalTok{(response, }\StringTok{"\textasciitilde{}"}\NormalTok{, }\FunctionTok{paste}\NormalTok{(predictors, }\AttributeTok{collapse =} \StringTok{"+"}\NormalTok{))),}
  \AttributeTok{data =}\NormalTok{ train\_data,}
  \AttributeTok{importance =} \ConstantTok{TRUE}
\NormalTok{)}
\NormalTok{rf\_model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
 randomForest(formula = as.formula(paste(response, "~", paste(predictors,      collapse = "+"))), data = train_data, importance = TRUE) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 3.45%
Confusion matrix:
          Adelie Chinstrap Gentoo class.error
Adelie       100         1      1  0.01960784
Chinstrap      4        42      1  0.10638298
Gentoo         0         1     82  0.01204819
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rf\_pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(rf\_model, }\AttributeTok{newdata =}\NormalTok{ test\_data)}
\NormalTok{rf\_pred }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        1         2         3         4         5         6         7         8 
   Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie 
        9        10        11        12        13        14        15        16 
   Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie 
       17        18        19        20        21        22        23        24 
   Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie 
       25        26        27        28        29        30        31        32 
   Adelie Chinstrap    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie 
       33        34        35        36        37        38        39        40 
   Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie 
       41        42        43        44        45        46        47        48 
   Adelie    Adelie    Adelie    Adelie    Gentoo    Gentoo    Gentoo    Gentoo 
       49        50        51        52        53        54        55        56 
   Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo 
       57        58        59        60        61        62        63        64 
   Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo 
       65        66        67        68        69        70        71        72 
   Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo 
       73        74        75        76        77        78        79        80 
   Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo 
       81        82        83        84        85        86        87        88 
Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap 
       89        90        91        92        93        94        95        96 
Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap 
       97        98        99       100       101 
Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap 
Levels: Adelie Chinstrap Gentoo
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rf\_cm }\OtherTok{\textless{}{-}} \FunctionTok{confusionMatrix}\NormalTok{(rf\_pred, test\_data}\SpecialCharTok{$}\NormalTok{species)}
\FunctionTok{print}\NormalTok{(rf\_cm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Confusion Matrix and Statistics

           Reference
Prediction  Adelie Chinstrap Gentoo
  Adelie        43         0      0
  Chinstrap      1        21      0
  Gentoo         0         0     36

Overall Statistics
                                          
               Accuracy : 0.9901          
                 95% CI : (0.9461, 0.9997)
    No Information Rate : 0.4356          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9846          
                                          
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: Adelie Class: Chinstrap Class: Gentoo
Sensitivity                 0.9773           1.0000        1.0000
Specificity                 1.0000           0.9875        1.0000
Pos Pred Value              1.0000           0.9545        1.0000
Neg Pred Value              0.9828           1.0000        1.0000
Prevalence                  0.4356           0.2079        0.3564
Detection Rate              0.4257           0.2079        0.3564
Detection Prevalence        0.4257           0.2178        0.3564
Balanced Accuracy           0.9886           0.9938        1.0000
\end{verbatim}

\section{Exercise}\label{exercise}

Compute the following measures manually and interpret them.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Sensitivity
\item
  Specificity
\item
  Prevalence
\item
  Positive Prediction Value (PPV)
\item
  Negative Prediction Value (NPV)
\item
  Detection rate
\item
  Detection prevalence
\item
  Balanced accuracy
\item
  Precision
\item
  Recall
\item
  F1 score
\end{enumerate}

\section{Receiver Operating Characteristic (ROC)
curves}\label{receiver-operating-characteristic-roc-curves}

ROC curves are normally for binary classification, but penguins\$species
has three classes. For multiclass ROC, we compute one-vs-all ROC curves.

It's a plot of:

\begin{itemize}
\item
  True Positive Rate (TPR / Sensitivity) on the y-axis
\item
  False Positive Rate (FPR = 1 - Specificity) on the x-axis
\item
  It shows the trade-off between sensitivity and specificity for
  different decision thresholds of a classifier.
\end{itemize}

\subsection{Metric Formula}\label{metric-formula}

True Positive Rate (Sensitivity)

\[TPR = TP / (TP + FN)\]

False Positive Rate

\[FPR = FP / (FP + TN)\]

Specificity

\[TN / (TN + FP) = 1 âˆ’ FPR\]

Where:

\(TP\) = True Positives

\(FP\) = False Positives

\(TN\) = True Negatives

\(FN\) = False Negatives

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Predict class probabilities}
\NormalTok{rf\_prob }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(rf\_model, }\AttributeTok{newdata =}\NormalTok{ test\_data, }\AttributeTok{type =} \StringTok{"prob"}\NormalTok{)}
\NormalTok{rf\_prob}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    Adelie Chinstrap Gentoo
1    0.978     0.022  0.000
2    0.924     0.076  0.000
3    0.990     0.010  0.000
4    0.976     0.024  0.000
5    0.972     0.028  0.000
6    0.994     0.006  0.000
7    0.994     0.006  0.000
8    0.964     0.036  0.000
9    0.994     0.006  0.000
10   0.984     0.016  0.000
11   0.718     0.280  0.002
12   0.998     0.002  0.000
13   0.998     0.002  0.000
14   0.994     0.006  0.000
15   0.952     0.048  0.000
16   0.966     0.034  0.000
17   1.000     0.000  0.000
18   0.996     0.004  0.000
19   0.966     0.034  0.000
20   0.992     0.008  0.000
21   0.894     0.068  0.038
22   0.984     0.016  0.000
23   0.990     0.010  0.000
24   1.000     0.000  0.000
25   0.974     0.026  0.000
26   0.262     0.734  0.004
27   0.802     0.196  0.002
28   0.864     0.088  0.048
29   0.978     0.022  0.000
30   0.934     0.060  0.006
31   0.980     0.020  0.000
32   0.750     0.250  0.000
33   0.986     0.014  0.000
34   0.998     0.002  0.000
35   0.982     0.018  0.000
36   0.968     0.032  0.000
37   0.988     0.012  0.000
38   0.998     0.002  0.000
39   1.000     0.000  0.000
40   0.980     0.020  0.000
41   0.986     0.014  0.000
42   0.990     0.010  0.000
43   0.958     0.042  0.000
44   0.898     0.060  0.042
45   0.000     0.000  1.000
46   0.000     0.004  0.996
47   0.000     0.000  1.000
48   0.000     0.000  1.000
49   0.002     0.006  0.992
50   0.006     0.014  0.980
51   0.000     0.000  1.000
52   0.000     0.012  0.988
53   0.000     0.000  1.000
54   0.000     0.004  0.996
55   0.016     0.008  0.976
56   0.000     0.000  1.000
57   0.000     0.000  1.000
58   0.014     0.004  0.982
59   0.016     0.026  0.958
60   0.000     0.000  1.000
61   0.002     0.008  0.990
62   0.004     0.026  0.970
63   0.002     0.000  0.998
64   0.000     0.000  1.000
65   0.000     0.000  1.000
66   0.000     0.000  1.000
67   0.000     0.004  0.996
68   0.000     0.000  1.000
69   0.000     0.000  1.000
70   0.000     0.000  1.000
71   0.000     0.000  1.000
72   0.000     0.002  0.998
73   0.000     0.000  1.000
74   0.000     0.000  1.000
75   0.002     0.006  0.992
76   0.012     0.020  0.968
77   0.000     0.000  1.000
78   0.000     0.000  1.000
79   0.000     0.000  1.000
80   0.000     0.000  1.000
81   0.018     0.982  0.000
82   0.032     0.968  0.000
83   0.040     0.960  0.000
84   0.022     0.978  0.000
85   0.162     0.838  0.000
86   0.052     0.944  0.004
87   0.038     0.958  0.004
88   0.070     0.920  0.010
89   0.030     0.966  0.004
90   0.204     0.794  0.002
91   0.030     0.966  0.004
92   0.036     0.940  0.024
93   0.026     0.968  0.006
94   0.334     0.506  0.160
95   0.022     0.976  0.002
96   0.250     0.456  0.294
97   0.004     0.990  0.006
98   0.030     0.966  0.004
99   0.034     0.960  0.006
100  0.202     0.722  0.076
101  0.328     0.474  0.198
attr(,"class")
[1] "matrix" "array"  "votes" 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Combine predicted probabilities with true labels for tidymodels}
\NormalTok{rf\_results }\OtherTok{\textless{}{-}}\NormalTok{ test\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(species) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_cols}\NormalTok{(}\FunctionTok{as.data.frame}\NormalTok{(rf\_prob))}
\NormalTok{rf\_results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 101 x 4
   species Adelie Chinstrap Gentoo
 * <fct>    <dbl>     <dbl>  <dbl>
 1 Adelie   0.978     0.022      0
 2 Adelie   0.924     0.076      0
 3 Adelie   0.99      0.01       0
 4 Adelie   0.976     0.024      0
 5 Adelie   0.972     0.028      0
 6 Adelie   0.994     0.006      0
 7 Adelie   0.994     0.006      0
 8 Adelie   0.964     0.036      0
 9 Adelie   0.994     0.006      0
10 Adelie   0.984     0.016      0
# i 91 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Compute ROC curves}
\CommentTok{\# roc\_curve() for multiclass expects:}
\CommentTok{\# truth column and all class probability columns}
\NormalTok{roc\_data }\OtherTok{\textless{}{-}}\NormalTok{ rf\_results }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{roc\_curve}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ species, Adelie, Chinstrap, Gentoo) }
\NormalTok{roc\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 129 x 4
   .level .threshold specificity sensitivity
   <chr>       <dbl>       <dbl>       <dbl>
 1 Adelie   -Inf           0               1
 2 Adelie      0           0               1
 3 Adelie      0.002       0.456           1
 4 Adelie      0.004       0.526           1
 5 Adelie      0.006       0.561           1
 6 Adelie      0.012       0.579           1
 7 Adelie      0.014       0.596           1
 8 Adelie      0.016       0.614           1
 9 Adelie      0.018       0.649           1
10 Adelie      0.022       0.667           1
# i 119 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Plot ROC curves}
\NormalTok{roc\_data }\SpecialCharTok{|\textgreater{}} \FunctionTok{autoplot}\NormalTok{()  }
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ch5_files/figure-pdf/unnamed-chunk-9-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Compute AUC for each class}
\NormalTok{auc\_data }\OtherTok{\textless{}{-}}\NormalTok{ rf\_results }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{roc\_auc}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ species, Adelie, Chinstrap, Gentoo)}
\NormalTok{auc\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 3
  .metric .estimator .estimate
  <chr>   <chr>          <dbl>
1 roc_auc hand_till      0.999
\end{verbatim}

\section{Illustration of How ROC Curves Are
Plotted}\label{illustration-of-how-roc-curves-are-plotted}

Let's do that for one species (Adelie) vs others, using predicted
probabilities from a Random Forest.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test\_data\_adelie }\OtherTok{\textless{}{-}}\NormalTok{ test\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{actual =} \FunctionTok{ifelse}\NormalTok{(species }\SpecialCharTok{==} \StringTok{"Adelie"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\NormalTok{test\_data\_adelie }\SpecialCharTok{|\textgreater{}} \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 101
Columns: 9
$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel~
$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Biscoe, ~
$ bill_length_mm    <dbl> 39.1, 39.5, 39.3, 36.6, 37.7, 35.9, 38.2, 38.8, 37.9~
$ bill_depth_mm     <dbl> 18.7, 17.4, 20.6, 17.8, 18.7, 19.2, 18.1, 17.2, 18.6~
$ flipper_length_mm <int> 181, 186, 190, 185, 180, 189, 185, 180, 172, 188, 18~
$ body_mass_g       <int> 3750, 3800, 3650, 3700, 3600, 3800, 3950, 3800, 3150~
$ sex               <fct> male, female, male, female, male, female, male, male~
$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007~
$ actual            <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~
\end{verbatim}

We can calculate TPR and FPR at different thresholds:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{thresholds }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.1}\NormalTok{)  }\CommentTok{\# thresholds from 0 to 1}
\NormalTok{roc\_manual }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{threshold =}\NormalTok{ thresholds, }\AttributeTok{TPR =} \ConstantTok{NA}\NormalTok{, }\AttributeTok{FPR =} \ConstantTok{NA}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \FunctionTok{seq\_along}\NormalTok{(thresholds)) \{}
\NormalTok{  thresh }\OtherTok{\textless{}{-}}\NormalTok{ thresholds[i]}
  
  \CommentTok{\# Predicted class based on threshold}
\NormalTok{  pred\_class }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(test\_data}\SpecialCharTok{$}\NormalTok{pred\_adelie }\SpecialCharTok{\textgreater{}=}\NormalTok{ thresh, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
  
  \CommentTok{\# Confusion matrix components}
\NormalTok{  TP }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(pred\_class }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ test\_data}\SpecialCharTok{$}\NormalTok{actual }\SpecialCharTok{==} \DecValTok{1}\NormalTok{)}
\NormalTok{  FP }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(pred\_class }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ test\_data}\SpecialCharTok{$}\NormalTok{actual }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)}
\NormalTok{  FN }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(pred\_class }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&}\NormalTok{ test\_data}\SpecialCharTok{$}\NormalTok{actual }\SpecialCharTok{==} \DecValTok{1}\NormalTok{)}
\NormalTok{  TN }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(pred\_class }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&}\NormalTok{ test\_data}\SpecialCharTok{$}\NormalTok{actual }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)}
  
  \CommentTok{\# Compute TPR and FPR}
\NormalTok{  roc\_manual}\SpecialCharTok{$}\NormalTok{TPR[i] }\OtherTok{\textless{}{-}}\NormalTok{ TP }\SpecialCharTok{/}\NormalTok{ (TP }\SpecialCharTok{+}\NormalTok{ FN)}
\NormalTok{  roc\_manual}\SpecialCharTok{$}\NormalTok{FPR[i] }\OtherTok{\textless{}{-}}\NormalTok{ FP }\SpecialCharTok{/}\NormalTok{ (FP }\SpecialCharTok{+}\NormalTok{ TN)}
\NormalTok{\}}

\NormalTok{roc\_manual}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   threshold TPR FPR
1        0.0 NaN NaN
2        0.1 NaN NaN
3        0.2 NaN NaN
4        0.3 NaN NaN
5        0.4 NaN NaN
6        0.5 NaN NaN
7        0.6 NaN NaN
8        0.7 NaN NaN
9        0.8 NaN NaN
10       0.9 NaN NaN
11       1.0 NaN NaN
\end{verbatim}

The NaN (Not a Number) appears because some thresholds may produce
divisions by zero, typically when (TP + FN) or (FP + TN) is zero. This
can happen if the classifier predicts all 0s or all 1s for extreme
thresholds (0 or 1).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{thresholds }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.1}\NormalTok{)  }\CommentTok{\# thresholds from 0 to 1}
\NormalTok{roc\_manual }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{threshold =}\NormalTok{ thresholds, }\AttributeTok{TPR =} \ConstantTok{NA}\NormalTok{, }\AttributeTok{FPR =} \ConstantTok{NA}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \FunctionTok{seq\_along}\NormalTok{(thresholds)) \{}
\NormalTok{  thresh }\OtherTok{\textless{}{-}}\NormalTok{ thresholds[i]}
  
  \CommentTok{\# Predicted class based on threshold}
\NormalTok{  pred\_class }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(test\_data}\SpecialCharTok{$}\NormalTok{pred\_adelie }\SpecialCharTok{\textgreater{}=}\NormalTok{ thresh, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
  
  \CommentTok{\# Confusion matrix components}
\NormalTok{  TP }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(pred\_class }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ test\_data}\SpecialCharTok{$}\NormalTok{actual }\SpecialCharTok{==} \DecValTok{1}\NormalTok{)}
\NormalTok{  FP }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(pred\_class }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ test\_data}\SpecialCharTok{$}\NormalTok{actual }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)}
\NormalTok{  FN }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(pred\_class }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&}\NormalTok{ test\_data}\SpecialCharTok{$}\NormalTok{actual }\SpecialCharTok{==} \DecValTok{1}\NormalTok{)}
\NormalTok{  TN }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(pred\_class }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&}\NormalTok{ test\_data}\SpecialCharTok{$}\NormalTok{actual }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)}
  
  \CommentTok{\# Compute TPR safely}
\NormalTok{  roc\_manual}\SpecialCharTok{$}\NormalTok{TPR[i] }\OtherTok{\textless{}{-}} \ControlFlowTok{if}\NormalTok{ ((TP }\SpecialCharTok{+}\NormalTok{ FN) }\SpecialCharTok{==} \DecValTok{0}\NormalTok{) }\DecValTok{0} \ControlFlowTok{else}\NormalTok{ TP }\SpecialCharTok{/}\NormalTok{ (TP }\SpecialCharTok{+}\NormalTok{ FN)}
\NormalTok{  roc\_manual}\SpecialCharTok{$}\NormalTok{FPR[i] }\OtherTok{\textless{}{-}} \ControlFlowTok{if}\NormalTok{ ((FP }\SpecialCharTok{+}\NormalTok{ TN) }\SpecialCharTok{==} \DecValTok{0}\NormalTok{) }\DecValTok{0} \ControlFlowTok{else}\NormalTok{ FP }\SpecialCharTok{/}\NormalTok{ (FP }\SpecialCharTok{+}\NormalTok{ TN)}
\NormalTok{\}}

\FunctionTok{print}\NormalTok{(roc\_manual)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   threshold TPR FPR
1        0.0   0   0
2        0.1   0   0
3        0.2   0   0
4        0.3   0   0
5        0.4   0   0
6        0.5   0   0
7        0.6   0   0
8        0.7   0   0
9        0.8   0   0
10       0.9   0   0
11       1.0   0   0
\end{verbatim}

\section{Variable Importance
Measures}\label{variable-importance-measures}

Random Forest provides two widely used importance metrics:

\textbf{(a) Mean Decrease in Gini (Gini Importance)}

\begin{verbatim}
Measures how much a variable reduces node impurity (Gini index) across all trees.

Variables used for highly discriminative splits receive larger importance values.

Higher value â†’ more important predictor.
\end{verbatim}

\textbf{(b) Mean Decrease in Accuracy (Permutation Importance)}

\begin{verbatim}
After the model is trained, the values of a variable are randomly permuted.

If the model accuracy drops significantly, the variable is important.

Reflects the predictive power of each variable.
\end{verbatim}

Interpretation:

Shows which features the random forest relied on most.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{importance}\NormalTok{(rf\_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                     Adelie Chinstrap    Gentoo MeanDecreaseAccuracy
bill_length_mm    95.948710 63.227999 14.989892             85.86134
bill_depth_mm     20.855953 17.906902 30.126735             35.12314
flipper_length_mm 18.476543 28.099211 28.799293             34.80012
body_mass_g        9.477884 19.466334 15.807238             21.34092
sex                7.976165  8.382336  6.158169             11.40692
                  MeanDecreaseGini
bill_length_mm            57.18023
bill_depth_mm             28.01945
flipper_length_mm         44.80369
body_mass_g               15.60508
sex                        1.49749
\end{verbatim}

\textbf{1. Mean Decrease in Accuracy (Permutation Importance)}

This measures how much the model's accuracy drops when the values of
each variable are randomly permuted. Higher values â†’ more important.

Interpretation:

bill\_length\_mm (85.86) is by far the most important variable.

â†’ Shuffling this variable causes a large drop in accuracy.

â†’ Species are strongly separated by bill length.

bill\_depth\_mm (35.12) and flipper\_length\_mm (34.80)

â†’ Moderately important.

â†’ They help the model differentiate species, but less than bill length.

body\_mass\_g (21.34)

â†’ Some importance.

â†’ Body mass varies between species but overlaps, so it is less
discriminative.

sex (11.41)

â†’ Least important.

â†’ Sex does not significantly differentiate species

\textbf{2. Mean Decrease in Gini (Impurity Importance)}

This measures how much each variable reduces node impurity in the
decision trees. Higher values â†’ variable used for strong, clean splits.

Interpretation:

bill\_length\_mm (57.18)

â†’ Most useful for creating pure nodes.

â†’ Confirms it is the single best predictor of species.

flipper\_length\_mm (44.80)

â†’ Also very important for clean splits.

bill\_depth\_mm (28.02)

â†’ Useful but less than the above.

body\_mass\_g (15.61)

â†’ Small contribution.

sex (1.50)

â†’ Barely contributes to splitting.

â†’ Almost irrelevant for species classification.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{varImpPlot}\NormalTok{(rf\_model,}
           \AttributeTok{main =} \StringTok{"Random Forest Variable Importance"}\NormalTok{,}
           \AttributeTok{pch =} \DecValTok{16}\NormalTok{,}
           \AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ch5_files/figure-pdf/unnamed-chunk-15-1.pdf}}

\section{ggplot2 Variable Importance Plot (Gini
Importance)}\label{ggplot2-variable-importance-plot-gini-importance}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\NormalTok{imp }\OtherTok{\textless{}{-}} \FunctionTok{importance}\NormalTok{(rf\_model)}
\NormalTok{imp\_df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{variable =} \FunctionTok{rownames}\NormalTok{(imp),}
  \AttributeTok{MeanDecreaseGini =}\NormalTok{ imp[, }\StringTok{"MeanDecreaseGini"}\NormalTok{]}
\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(imp\_df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{reorder}\NormalTok{(variable, MeanDecreaseGini),}
                   \AttributeTok{y =}\NormalTok{ MeanDecreaseGini)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{fill =} \StringTok{"steelblue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Random Forest Variable Importance (Mean Decrease Gini)"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Predictor"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Importance (Gini)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{14}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ch5_files/figure-pdf/unnamed-chunk-16-1.pdf}}

\section{ggplot2 Variable Importance Plot (Permutation
Importance)}\label{ggplot2-variable-importance-plot-permutation-importance}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imp2\_df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{variable =} \FunctionTok{rownames}\NormalTok{(imp),}
  \AttributeTok{MeanDecreaseAccuracy =}\NormalTok{ imp[, }\StringTok{"MeanDecreaseAccuracy"}\NormalTok{]}
\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(imp2\_df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{reorder}\NormalTok{(variable, MeanDecreaseAccuracy),}
                    \AttributeTok{y =}\NormalTok{ MeanDecreaseAccuracy)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{fill =} \StringTok{"darkgreen"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Random Forest Variable Importance (Permutation Importance)"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Predictor"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Importance (Mean Decrease Accuracy)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{14}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ch5_files/figure-pdf/unnamed-chunk-17-1.pdf}}

\bookmarksetup{startatroot}

\chapter{k-fold Cross Validation
Approaches}\label{k-fold-cross-validation-approaches}

\section{Load Packages}\label{load-packages}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(randomForest)}
\FunctionTok{library}\NormalTok{(pROC)}
\FunctionTok{library}\NormalTok{(caret)}
\FunctionTok{library}\NormalTok{(palmerpenguins)}
\end{Highlighting}
\end{Shaded}

\section{Prepare data}\label{prepare-data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\OtherTok{\textless{}{-}}\NormalTok{ penguins }\SpecialCharTok{|\textgreater{}} \FunctionTok{na.omit}\NormalTok{()}
\NormalTok{penguins}\SpecialCharTok{$}\NormalTok{species }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(penguins}\SpecialCharTok{$}\NormalTok{species)}
\end{Highlighting}
\end{Shaded}

\section{Create k-fold cross
validation}\label{create-k-fold-cross-validation}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{folds }\OtherTok{\textless{}{-}} \FunctionTok{createFolds}\NormalTok{(penguins}\SpecialCharTok{$}\NormalTok{species, }\AttributeTok{k =} \DecValTok{5}\NormalTok{)}
\NormalTok{folds}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$Fold1
 [1]   8  13  16  20  23  27  36  51  56  57  58  60  63  64  87  90  91  94  96
[20] 106 110 112 118 124 125 130 134 138 139 151 152 154 156 166 181 182 190 200
[39] 204 206 211 213 222 234 235 236 238 240 243 244 254 257 263 266 273 277 278
[58] 284 291 302 309 310 312 313 317 324

$Fold2
 [1]  10  12  14  19  22  30  33  43  46  47  54  61  68  71  72  74  78  82  92
[20]  97 102 104 107 108 111 119 120 123 132 147 149 158 160 161 169 174 178 180
[39] 191 192 198 199 201 202 215 219 226 231 237 242 248 250 259 267 268 270 274
[58] 280 287 300 316 319 320 322 332 333

$Fold3
 [1]   3   4   5  15  17  18  25  29  32  35  37  40  44  45  48  66  70  86  89
[20]  93  99 105 116 122 129 131 136 140 141 144 157 165 172 177 183 184 186 187
[39] 194 195 197 205 207 209 210 212 217 218 220 228 229 232 262 276 281 282 285
[58] 288 289 292 293 298 304 315 323 325 329

$Fold4
 [1]   1   6   9  11  24  26  28  38  39  41  52  62  67  69  79  81  83  84  85
[20]  88  98 100 109 115 128 135 143 145 146 153 155 159 164 167 168 175 188 193
[39] 196 208 216 221 223 224 227 230 233 241 247 256 258 261 265 269 271 286 290
[58] 299 301 303 305 306 307 308 311 321 331

$Fold5
 [1]   2   7  21  31  34  42  49  50  53  55  59  65  73  75  76  77  80  95 101
[20] 103 113 114 117 121 126 127 133 137 142 148 150 162 163 170 171 173 176 179
[39] 185 189 203 214 225 239 245 246 249 251 252 253 255 260 264 272 275 279 283
[58] 294 295 296 297 314 318 326 327 328 330
\end{verbatim}

\section{Run random forest on each
fold}\label{run-random-forest-on-each-fold}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Fold =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{,}
  \AttributeTok{Accuracy =} \ConstantTok{NA}\NormalTok{,}
  \AttributeTok{Sensitivity =} \ConstantTok{NA}\NormalTok{,}
  \AttributeTok{Specificity =} \ConstantTok{NA}\NormalTok{,}
  \AttributeTok{AUC =} \ConstantTok{NA}
\NormalTok{)}

\NormalTok{roc\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{) \{}
  
\NormalTok{  test\_idx  }\OtherTok{\textless{}{-}}\NormalTok{ folds[[i]]}
\NormalTok{  train\_set }\OtherTok{\textless{}{-}}\NormalTok{ penguins[}\SpecialCharTok{{-}}\NormalTok{test\_idx, ]}
\NormalTok{  test\_set  }\OtherTok{\textless{}{-}}\NormalTok{ penguins[test\_idx, ]}
  
  \CommentTok{\# Fit RF}
\NormalTok{  rf\_model }\OtherTok{\textless{}{-}} \FunctionTok{randomForest}\NormalTok{(}
\NormalTok{    species }\SpecialCharTok{\textasciitilde{}}\NormalTok{ bill\_length\_mm }\SpecialCharTok{+}\NormalTok{ bill\_depth\_mm }\SpecialCharTok{+}
\NormalTok{      flipper\_length\_mm }\SpecialCharTok{+}\NormalTok{ body\_mass\_g }\SpecialCharTok{+}\NormalTok{ sex,}
    \AttributeTok{data =}\NormalTok{ train\_set,}
    \AttributeTok{ntree =} \DecValTok{500}\NormalTok{,}
    \AttributeTok{mtry =} \DecValTok{3}\NormalTok{,}
    \AttributeTok{importance =} \ConstantTok{TRUE}
\NormalTok{  )}
  
  \CommentTok{\# Predict class}
\NormalTok{  pred\_class }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(rf\_model, test\_set)}
  
  \CommentTok{\# Predict probabilities}
\NormalTok{  pred\_prob }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(rf\_model, test\_set, }\AttributeTok{type =} \StringTok{"prob"}\NormalTok{)}
  
  \CommentTok{\# Confusion matrix}
\NormalTok{  cm }\OtherTok{\textless{}{-}} \FunctionTok{confusionMatrix}\NormalTok{(pred\_class, test\_set}\SpecialCharTok{$}\NormalTok{species)}
  
\NormalTok{  results}\SpecialCharTok{$}\NormalTok{Accuracy[i]    }\OtherTok{\textless{}{-}}\NormalTok{ cm}\SpecialCharTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{]}
\NormalTok{  results}\SpecialCharTok{$}\NormalTok{Sensitivity[i] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(cm}\SpecialCharTok{$}\NormalTok{byClass[, }\StringTok{"Sensitivity"}\NormalTok{], }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  results}\SpecialCharTok{$}\NormalTok{Specificity[i] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(cm}\SpecialCharTok{$}\NormalTok{byClass[, }\StringTok{"Specificity"}\NormalTok{], }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
  
  \CommentTok{\# Handâ€“Till AUC for MULTICLASS}
\NormalTok{  roc\_obj }\OtherTok{\textless{}{-}} \FunctionTok{multiclass.roc}\NormalTok{(}\AttributeTok{response =}\NormalTok{ test\_set}\SpecialCharTok{$}\NormalTok{species, }\AttributeTok{predictor =}\NormalTok{ pred\_prob)}
\NormalTok{  results}\SpecialCharTok{$}\NormalTok{AUC[i] }\OtherTok{\textless{}{-}} \FunctionTok{auc}\NormalTok{(roc\_obj)}
  
\NormalTok{  roc\_list[[i]] }\OtherTok{\textless{}{-}}\NormalTok{ roc\_obj}
\NormalTok{\}}

\NormalTok{results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Fold  Accuracy Sensitivity Specificity       AUC
1    1 0.9545455   0.9631226   0.9794849 0.9990790
2    2 0.9848485   0.9885057   0.9937107 0.9986737
3    3 0.9850746   0.9888889   0.9937107 1.0000000
4    4 0.9850746   0.9761905   0.9912281 1.0000000
5    5 0.9701493   0.9523810   0.9824561 0.9958949
\end{verbatim}

\section{When using Random Forest with Cross-Validation, what is the
final
model?}\label{when-using-random-forest-with-cross-validation-what-is-the-final-model}

During k-fold cross-validation, you train K different random forest
models, each on a different subset of the data.

But none of those K models is your final model.

The purpose of Cross Validation in Random Forest

âœ”ï¸ Evaluate multiple hyperparameter combinations

For example:

mtry (number of variables randomly chosen at each split)

ntree (number of trees)

nodesize

maxnodes

âœ”ï¸ Measure model performance for each hyperparameter set

Using:

Accuracy

AUC

Sensitivity / Specificity

Kappa

Log-loss (if needed)

âœ”ï¸ Choose the best-performing hyperparameter combination

(Usually the one giving highest CV accuracy or AUC.)

ðŸŽ¯ Then what?

After identifying the best hyperparameters

ðŸ‘‰ You retrain one final random forest model on the entire dataset using
ONLY the best hyperparameters.

\section{Example}\label{example-2}

\subsection{Hyperparameter Grid}\label{hyperparameter-grid}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grid }\OtherTok{\textless{}{-}} \FunctionTok{expand.grid}\NormalTok{(}
  \AttributeTok{mtry  =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{),}
  \AttributeTok{ntree =} \FunctionTok{c}\NormalTok{(}\DecValTok{300}\NormalTok{, }\DecValTok{500}\NormalTok{, }\DecValTok{800}\NormalTok{)}
\NormalTok{)}
\NormalTok{grid}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  mtry ntree
1    2   300
2    3   300
3    4   300
4    2   500
5    3   500
6    4   500
7    2   800
8    3   800
9    4   800
\end{verbatim}

\subsection{Set up 5-fold
cross-validation}\label{set-up-5-fold-cross-validation}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{folds }\OtherTok{\textless{}{-}} \FunctionTok{createFolds}\NormalTok{(penguins}\SpecialCharTok{$}\NormalTok{species, }\AttributeTok{k =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Run cross-validation for each hyperparameter
combination}\label{run-cross-validation-for-each-hyperparameter-combination}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cv\_results }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{mtry =}\NormalTok{ grid}\SpecialCharTok{$}\NormalTok{mtry,}
  \AttributeTok{ntree =}\NormalTok{ grid}\SpecialCharTok{$}\NormalTok{ntree,}
  \AttributeTok{Accuracy =} \ConstantTok{NA}
\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(grid)) \{}
  
\NormalTok{  m }\OtherTok{\textless{}{-}}\NormalTok{ grid}\SpecialCharTok{$}\NormalTok{mtry[i]}
\NormalTok{  t }\OtherTok{\textless{}{-}}\NormalTok{ grid}\SpecialCharTok{$}\NormalTok{ntree[i]}
  
\NormalTok{  acc\_vec }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{()}
  
  \ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{) \{}
    
\NormalTok{    test\_idx  }\OtherTok{\textless{}{-}}\NormalTok{ folds[[k]]}
\NormalTok{    train\_set }\OtherTok{\textless{}{-}}\NormalTok{ penguins[}\SpecialCharTok{{-}}\NormalTok{test\_idx, ]}
\NormalTok{    test\_set  }\OtherTok{\textless{}{-}}\NormalTok{ penguins[test\_idx, ]}
    
    \CommentTok{\# Fit model}
\NormalTok{    rf\_model }\OtherTok{\textless{}{-}} \FunctionTok{randomForest}\NormalTok{(}
\NormalTok{      species }\SpecialCharTok{\textasciitilde{}}\NormalTok{ bill\_length\_mm }\SpecialCharTok{+}\NormalTok{ bill\_depth\_mm }\SpecialCharTok{+}
\NormalTok{        flipper\_length\_mm }\SpecialCharTok{+}\NormalTok{ body\_mass\_g }\SpecialCharTok{+}\NormalTok{ sex,}
      \AttributeTok{data =}\NormalTok{ train\_set,}
      \AttributeTok{mtry =}\NormalTok{ m,}
      \AttributeTok{ntree =}\NormalTok{ t}
\NormalTok{    )}
    
    \CommentTok{\# Predict}
\NormalTok{    pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(rf\_model, test\_set)}
    
    \CommentTok{\# Accuracy for this fold}
\NormalTok{    acc }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(pred }\SpecialCharTok{==}\NormalTok{ test\_set}\SpecialCharTok{$}\NormalTok{species)}
\NormalTok{    acc\_vec }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(acc\_vec, acc)}
\NormalTok{  \}}
  
  \CommentTok{\# Store mean CV accuracy}
\NormalTok{  cv\_results}\SpecialCharTok{$}\NormalTok{Accuracy[i] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(acc\_vec)}
\NormalTok{\}}

\NormalTok{cv\_results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  mtry ntree  Accuracy
1    2   300 0.9819539
2    3   300 0.9819539
3    4   300 0.9789236
4    2   500 0.9819539
5    3   500 0.9819539
6    4   500 0.9789236
7    2   800 0.9819539
8    3   800 0.9789236
9    4   800 0.9789236
\end{verbatim}

\subsection{Select the best
hyperparameters}\label{select-the-best-hyperparameters}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{best\_row }\OtherTok{\textless{}{-}}\NormalTok{ cv\_results[}\FunctionTok{which.max}\NormalTok{(cv\_results}\SpecialCharTok{$}\NormalTok{Accuracy), ]}
\NormalTok{best\_row}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  mtry ntree  Accuracy
1    2   300 0.9819539
\end{verbatim}

\bookmarksetup{startatroot}

\chapter{Extreme Gradient Boosting
(XGBoost)}\label{extreme-gradient-boosting-xgboost}

Extreme Gradient Boosting (XGBoost) is a machine learning algorithm that
uses gradient boosted decision trees. It is part of a new generation of
algorithms that are designed to be highly accurate.

XGBoost improves accuracy mainly by reducing overfitting during
training. This is achieved through its objective function, which has two
parts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Loss function -- measures how far the model's predictions are from the
  actual values.
\item
  Regularization -- controls the complexity of the model, helping to
  prevent overfitting.
\end{enumerate}

By combining these, XGBoost builds a model that is both accurate and
generalizes well to new data.

\section{Without cross validation}\label{without-cross-validation}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Install packages if not already installed}
\CommentTok{\# install.packages("xgboost")}
\CommentTok{\# install.packages("caret")}
\CommentTok{\# install.packages("Matrix")}

\FunctionTok{library}\NormalTok{(xgboost)}
\FunctionTok{library}\NormalTok{(caret)}
\FunctionTok{library}\NormalTok{(Matrix)}

\CommentTok{\# Example dataset: Iris (binary classification: setosa vs others)}
\FunctionTok{data}\NormalTok{(iris)}
\NormalTok{iris}\SpecialCharTok{$}\NormalTok{label }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{Species }\SpecialCharTok{==} \StringTok{"setosa"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{\# binary label}
\NormalTok{iris}\SpecialCharTok{$}\NormalTok{Species }\OtherTok{\textless{}{-}} \ConstantTok{NULL}  \CommentTok{\# remove original factor}

\CommentTok{\# Split data into training and test sets}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{train\_index }\OtherTok{\textless{}{-}} \FunctionTok{createDataPartition}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{label, }\AttributeTok{p =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{list =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{train\_data }\OtherTok{\textless{}{-}}\NormalTok{ iris[train\_index, ]}
\NormalTok{test\_data }\OtherTok{\textless{}{-}}\NormalTok{ iris[}\SpecialCharTok{{-}}\NormalTok{train\_index, ]}

\CommentTok{\# Convert data to matrix form}
\NormalTok{train\_matrix }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(train\_data[, }\SpecialCharTok{{-}}\FunctionTok{ncol}\NormalTok{(train\_data)])}
\NormalTok{train\_label }\OtherTok{\textless{}{-}}\NormalTok{ train\_data}\SpecialCharTok{$}\NormalTok{label}
\NormalTok{test\_matrix }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(test\_data[, }\SpecialCharTok{{-}}\FunctionTok{ncol}\NormalTok{(test\_data)])}
\NormalTok{test\_label }\OtherTok{\textless{}{-}}\NormalTok{ test\_data}\SpecialCharTok{$}\NormalTok{label}

\CommentTok{\# Create DMatrix objects (XGBoost format)}
\NormalTok{dtrain }\OtherTok{\textless{}{-}} \FunctionTok{xgb.DMatrix}\NormalTok{(}\AttributeTok{data =}\NormalTok{ train\_matrix, }\AttributeTok{label =}\NormalTok{ train\_label)}
\NormalTok{dtest }\OtherTok{\textless{}{-}} \FunctionTok{xgb.DMatrix}\NormalTok{(}\AttributeTok{data =}\NormalTok{ test\_matrix, }\AttributeTok{label =}\NormalTok{ test\_label)}

\CommentTok{\# Set parameters}
\NormalTok{params }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
  \AttributeTok{booster =} \StringTok{"gbtree"}\NormalTok{,}
  \AttributeTok{objective =} \StringTok{"binary:logistic"}\NormalTok{,}
  \AttributeTok{eval\_metric =} \StringTok{"logloss"}\NormalTok{,  }\CommentTok{\# or "error"}
  \AttributeTok{eta =} \FloatTok{0.1}\NormalTok{,  }\CommentTok{\# learning rate}
  \AttributeTok{max\_depth =} \DecValTok{3}\NormalTok{,}
  \AttributeTok{min\_child\_weight =} \DecValTok{1}\NormalTok{,}
  \AttributeTok{subsample =} \FloatTok{0.8}\NormalTok{,}
  \AttributeTok{colsample\_bytree =} \FloatTok{0.8}
\NormalTok{)}

\CommentTok{\# Train the model}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{xgb\_model }\OtherTok{\textless{}{-}} \FunctionTok{xgb.train}\NormalTok{(}
  \AttributeTok{params =}\NormalTok{ params,}
  \AttributeTok{data =}\NormalTok{ dtrain,}
  \AttributeTok{nrounds =} \DecValTok{100}\NormalTok{,}
  \AttributeTok{watchlist =} \FunctionTok{list}\NormalTok{(}\AttributeTok{train =}\NormalTok{ dtrain, }\AttributeTok{test =}\NormalTok{ dtest),}
  \AttributeTok{early\_stopping\_rounds =} \DecValTok{10}\NormalTok{,}
  \AttributeTok{print\_every\_n =} \DecValTok{10}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Multiple eval metrics are present. Will use test_logloss for early stopping.
Will train until test_logloss hasn't improved in 10 rounds.

[1] train-logloss:0.552090  test-logloss:0.552090 
[11]    train-logloss:0.179963  test-logloss:0.179392 
[21]    train-logloss:0.074191  test-logloss:0.073715 
[31]    train-logloss:0.036053  test-logloss:0.035705 
[41]    train-logloss:0.021509  test-logloss:0.021298 
[51]    train-logloss:0.019402  test-logloss:0.019212 
Stopping. Best iteration:
[56]    train-logloss:0.019402  test-logloss:0.019212

[56]    train-logloss:0.019402  test-logloss:0.019212 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Make predictions}
\NormalTok{pred\_prob }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(xgb\_model, dtest)}
\NormalTok{pred\_label }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(pred\_prob }\SpecialCharTok{\textgreater{}} \FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}

\CommentTok{\# Evaluate}
\FunctionTok{confusionMatrix}\NormalTok{(}\FunctionTok{factor}\NormalTok{(pred\_label), }\FunctionTok{factor}\NormalTok{(test\_label))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Confusion Matrix and Statistics

          Reference
Prediction  0  1
         0 20  0
         1  0 10
                                     
               Accuracy : 1          
                 95% CI : (0.8843, 1)
    No Information Rate : 0.6667     
    P-Value [Acc > NIR] : 5.215e-06  
                                     
                  Kappa : 1          
                                     
 Mcnemar's Test P-Value : NA         
                                     
            Sensitivity : 1.0000     
            Specificity : 1.0000     
         Pos Pred Value : 1.0000     
         Neg Pred Value : 1.0000     
             Prevalence : 0.6667     
         Detection Rate : 0.6667     
   Detection Prevalence : 0.6667     
      Balanced Accuracy : 1.0000     
                                     
       'Positive' Class : 0          
                                     
\end{verbatim}

\section{With cross validation}\label{with-cross-validation}

\bookmarksetup{startatroot}

\chapter{Exercise: MCQ}\label{exercise-mcq}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Which of the following statements is true about a Decision Tree?
\end{enumerate}

\begin{verbatim}
A)  It is an ensemble method combining multiple models

B)  It splits the data based on feature values to create a tree structure

C)  It always gives better accuracy than Random Forest

D)  It cannot be used for classification tasks
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Bagging (Bootstrap Aggregating) primarily helps to:
\end{enumerate}

\begin{verbatim}
A)  Reduce bias in the model

B)  Reduce variance in the model

C)  Increase the depth of a single decision tree

D)  Select the most important feature only
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Random Forest differs from Bagging mainly because it:
\end{enumerate}

\begin{verbatim}
A)  Uses boosting instead of bagging

B)  Randomly selects a subset of features at each split

C)  Builds only a single decision tree

D)  Increases bias intentionally
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  AdaBoost works by:
\end{enumerate}

\begin{verbatim}
A)  Training multiple weak learners sequentially, focusing on misclassified points

B)  Training multiple models independently and averaging predictions

C)  Using only deep decision trees as base learners

D)  Randomly dropping features to reduce correlation
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Which of the following is true regarding Random Forest?
\end{enumerate}

\begin{verbatim}
A)  It is sensitive to outliers

B)  It is a type of boosting method

C)  It reduces overfitting compared to a single decision tree

D)  It cannot handle categorical variables
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  In AdaBoost, the weights of misclassified observations:
\end{enumerate}

\begin{verbatim}
A)  Decrease in the next iteration

B)  Remain the same

C)  Increase in the next iteration

D)  are ignored
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  Which method is most likely to reduce both variance and overfitting?
\end{enumerate}

\begin{verbatim}
A)  Single Decision Tree

B)  Bagging

C)  Random Forest

D)  AdaBoost
\end{verbatim}

\bookmarksetup{startatroot}

\chapter{Support Vector Machine}\label{support-vector-machine}

\section{What is a hyperplane?}\label{what-is-a-hyperplane}

A hyperplane is a flat, dividing surface in a vector space that's one
dimension smaller than the space it's in; it generalizes concepts like
lines (in 2D) and planes (in 3D) to higher dimensions, acting as a
decision boundary in machine learning for separating data into classes.

In-class

\begin{itemize}
\item
  Hyperplane in 2D
\item
  Hyperplane in 3D
\end{itemize}

\href{https://www.enjoyalgorithms.com/blog/support-vector-machine-in-ml}{Reading}

\pandocbounded{\includegraphics[keepaspectratio]{ch9_files/figure-pdf/unnamed-chunk-1-1.pdf}}

\section{Mathematical Definition of
Hyperplanes}\label{mathematical-definition-of-hyperplanes}

In-class

\section{Classification Using a Separating
Hyperplane}\label{classification-using-a-separating-hyperplane}

One can easily determine on which side of the hyperplane a point lies by
simply calculating the sign.

\section{There can be multiple number of hyperplanes. Which is the
best?}\label{there-can-be-multiple-number-of-hyperplanes.-which-is-the-best}

In-class diagram

\section{Criteria for choosing the best
hyperplane}\label{criteria-for-choosing-the-best-hyperplane}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Maximum margin (SVM approach):

  Pick the hyperplane where the distance to the nearest points on both
  sides is maximized.

  Ensures better generalization.
\item
  Minimizing classification error:

  Count the number of misclassified points for each hyperplane.

  Choose the hyperplane with the fewest misclassifications.
\item
  Other metrics (if regression or probabilistic separation):

  Accuracy, Precision, Recall, F1-score for classification.

  Mean Squared Error (MSE) for regression.
\end{enumerate}

\section{Maximal Margin Hyperplane (Optimal Separating
Hyperplane)}\label{maximal-margin-hyperplane-optimal-separating-hyperplane}

\textbf{Concept:}

For linearly separable data:

There are many hyperplanes that can separate two classes.

The maximum margin hyperplane is the one that maximizes the distance
(margin) to the nearest points of both classes.

\pandocbounded{\includegraphics[keepaspectratio]{ch9_files/figure-pdf/unnamed-chunk-2-1.pdf}}

Imagine a 2D scenario:

You have two groups of points on a piece of paper:

Group A (e.g., red dots)

Group B (e.g., green dots)

You want to draw a straight line that separates the two groups.

There are many possible lines that can separate them.

\textbf{The ``maximum margin'' idea:}

Instead of just picking any line, you want the line to be as far away as
possible from the nearest points in each group.

\textbf{Why?}

This gives more ``breathing room'' for your decision.

Makes the classification more robust to new points or slight noise.

The distance between the line and the nearest points of each group is
called the margin.

The Maximum Margin Hyperplane is the line that makes this margin as wide
as possible.

\textbf{Simple analogy:}

Think of a road between two rows of parked cars:

There's many possible paths you could drive through.

You choose the path that keeps the largest distance from both rows, so
you have maximum safety space.

That ``safest path'' is like the maximum margin hyperplane.

\section{Maximal Margin Hyperplane: Optimization
problem}\label{maximal-margin-hyperplane-optimization-problem}

In-class

\href{https://python.plainenglish.io/support-vector-machine-svm-clearly-explained-d9db9123b7ac}{Reading}

\section{Limitations: Maximal Margin
Hyperplane}\label{limitations-maximal-margin-hyperplane}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\item
  Sensitive to outliers

  Example: One point far away from its class can completely change the
  hyperplane.

  Since the MMH tries to maximize margin while keeping all points
  correctly classified, even a single outlier may force the hyperplane
  to tilt badly.

  This can reduce the generalization to new data.
\item
  Cannot handle non-linearly separable data

  If the two classes overlap, there is no hyperplane that separates them
  perfectly.

  Hard-margin SVM cannot be applied directly in this case.
\item
  Overfitting risk

  If we try to force perfect separation with noisy data, the MMH may fit
  very tightly to specific points, which is not robust.
\end{enumerate}

\section{Soft margin classifier or Support Vector Classifier
(SVC)}\label{soft-margin-classifier-or-support-vector-classifier-svc}

Like the Maximum Margin Classifier (MMC), the main goal of the Support
Vector Classifier (SVC) is to find the best boundary that separates
classes. However, unlike the MMC, which needs a perfect separation, the
SVC allows a few points to be misclassified. This makes it more flexible
and robust, especially when the data is noisy or the classes overlap.

\section{Support Vector Classifier: Optimization
Problem}\label{support-vector-classifier-optimization-problem}

In-class

\section{Support Vector Machine (SVM): Kernel Trick (Non-linear
boundary)}\label{support-vector-machine-svm-kernel-trick-non-linear-boundary}

\begin{itemize}
\item
  Sometimes, the classes in your data cannot be separated by a straight
  line (not linearly separable).
\item
  The kernel trick lets SVM draw a straight line in a higher-dimensional
  space, which corresponds to a curved or non-linear boundary in the
  original space.
\end{itemize}

In other words

\begin{itemize}
\tightlist
\item
  Start simple in low dimensions. If the data cannot be separated, lift
  it to a higher-dimensional space. Then find the widest possible road
  (support vector classifier) that separates the classes, allowing some
  flexibility if needed.
\end{itemize}

\href{https://www.enjoyalgorithms.com/blog/support-vector-machine-in-ml}{Diagrams}

\section{Summary}\label{summary-1}

A Support Vector Machine (SVM) is a supervised machine learning
algorithm used for classification and regression. Its main goal is to
find the best boundary (hyperplane) that separates different classes of
data.

For linearly separable data: SVM finds the Maximum Margin Hyperplane
(MMH), which maximizes the distance to the nearest points of each class.

For real-world or noisy data: SVM uses a soft margin (Support Vector
Classifier) to allow some misclassifications while still maximizing the
margin.

For non-linear separation: SVM can use kernels to transform the data
into a higher-dimensional space where a linear boundary can separate
classes.

\section{SVM with R}\label{svm-with-r}

\subsection{Example 1}\label{example-1-1}

Data

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Small dataset}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{),}
  \AttributeTok{class =} \FunctionTok{factor}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{)}

\NormalTok{df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  x y class
1 1 2     0
2 2 1     0
3 3 4     0
4 4 5     1
5 5 4     1
6 6 6     1
\end{verbatim}

Fit SVM

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(e1071)}
\CommentTok{\# Fit linear SVM}
\NormalTok{svm\_model }\OtherTok{\textless{}{-}} \FunctionTok{svm}\NormalTok{(class }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ y, }\AttributeTok{data =}\NormalTok{ df, }\AttributeTok{kernel =} \StringTok{"linear"}\NormalTok{, }\AttributeTok{cost =} \DecValTok{1}\NormalTok{)}
\CommentTok{\# Print model summary}
\FunctionTok{summary}\NormalTok{(svm\_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
svm(formula = class ~ x + y, data = df, kernel = "linear", cost = 1)


Parameters:
   SVM-Type:  C-classification 
 SVM-Kernel:  linear 
       cost:  1 

Number of Support Vectors:  5

 ( 3 2 )


Number of Classes:  2 

Levels: 
 0 1
\end{verbatim}

Predictions

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Predict on training data}
\NormalTok{pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(svm\_model, df)}

\CommentTok{\# Compare predictions with true labels}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{pred }\OtherTok{\textless{}{-}}\NormalTok{ pred}
\NormalTok{df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  x y class pred
1 1 2     0    0
2 2 1     0    0
3 3 4     0    1
4 4 5     1    1
5 5 4     1    1
6 6 6     1    1
\end{verbatim}

Accuracy Measures

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Confusion matrix}
\FunctionTok{table}\NormalTok{(}\AttributeTok{Predicted =}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{pred, }\AttributeTok{Actual =}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         Actual
Predicted 0 1
        0 2 0
        1 1 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Accuracy}
\NormalTok{accuracy }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{pred }\SpecialCharTok{==}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{class)}
\NormalTok{accuracy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.8333333
\end{verbatim}

\subsection{Example 2}\label{example-2-1}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Loading necessary libraries}
\FunctionTok{library}\NormalTok{(e1071)}
\FunctionTok{library}\NormalTok{(ggplot2)}

\CommentTok{\# Preparing data}
\FunctionTok{data}\NormalTok{(iris)}
\NormalTok{subset\_iris }\OtherTok{\textless{}{-}}\NormalTok{ iris[iris}\SpecialCharTok{$}\NormalTok{Species }\SpecialCharTok{!=} \StringTok{\textquotesingle{}setosa\textquotesingle{}}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{)]}

\CommentTok{\# Building SVM model}
\NormalTok{svm\_model }\OtherTok{\textless{}{-}} \FunctionTok{svm}\NormalTok{(Species}\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data =}\NormalTok{ subset\_iris, }\AttributeTok{method=}\StringTok{"C{-}classification"}\NormalTok{, }
                 \AttributeTok{kernel =} \StringTok{"linear"}\NormalTok{)}

\CommentTok{\# Plotting with base R}
\FunctionTok{plot}\NormalTok{(svm\_model, subset\_iris)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ch9_files/figure-pdf/unnamed-chunk-7-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svm\_data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(subset\_iris, }\AttributeTok{fit=}\FunctionTok{predict}\NormalTok{(svm\_model, subset\_iris))}
\FunctionTok{ggplot}\NormalTok{(svm\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{y=}\NormalTok{Sepal.Length, }\AttributeTok{x=}\NormalTok{Sepal.Width, }\AttributeTok{color=}\NormalTok{Species)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data=}\NormalTok{svm\_data[svm\_model}\SpecialCharTok{$}\NormalTok{index,], }\FunctionTok{aes}\NormalTok{(}\AttributeTok{y=}\NormalTok{Sepal.Length, }\AttributeTok{x=}\NormalTok{Sepal.Width), }
               \AttributeTok{shape=}\DecValTok{8}\NormalTok{, }\AttributeTok{size=}\DecValTok{3}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"SVM Classification with ggplot2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{ch9_files/figure-pdf/unnamed-chunk-7-2.pdf}}

When you train an SVM model in R using the e1071::svm() function, the
algorithm tries to find the optimal hyperplane that separates the
classes. Not all points in your dataset influence this hyperplane ---
only some of them lie on the margin or are close to it. These critical
points are called support vectors. \texttt{svm\_model\$index} contains
the row numbers (indices) of the training data that are support vectors.
These support vectors are the only points that define the SVM decision
boundary. Points far from the margin do not affect the hyperplane.

\section{Multi-class classification using Support Vector
Machines}\label{multi-class-classification-using-support-vector-machines}

Support Vector Machines (SVMs) are inherently binary classifiers
designed to find an optimal hyperplane separating two classes. To adapt
SVMs for multi-class classification (problems with three or more
classes), the problem is typically decomposed into multiple binary
classification tasks using specific strategies.

The two most common methods for achieving multi-class classification
with SVMs are:

\textbf{1. One-vs-Rest (OvR) or One-vs-All (OvA)}

In the OvR approach, a separate binary SVM model is trained for each
class.

Training Phase: If there are (N) classes, (N) different SVM classifiers
are trained. For classifier (i), all instances of class (i) are treated
as positive examples, and all instances from the other (N-1) classes are
grouped together as negative examples.Prediction Phase: For a new data
point, all (N) classifiers run their predictions. The classifier that
outputs the highest score or confidence determines the final class
label.

Pros/Cons: This method is computationally efficient in training as it
only requires fitting (N) models. However, it can suffer from class
imbalance because the ``rest'' class is usually much larger than the
single ``one'' class. It's often preferred when computational efficiency
is a priority.

\textbf{2. One-vs-One (OvO)}

The OvO method trains a binary SVM for every possible pair of classes.
For (N) classes, this means training (N*(N-1)/2) classifiers, each using
data only from the two classes it is trained to distinguish. During
prediction, each classifier votes for one of its two classes, and the
data point is assigned to the class with the most votes. OvO is more
computationally intensive in training than OvR but can offer higher
accuracy through focused pairwise comparisons. It scales well with the
number of training samples but not as well with a large number of
classes.

\section{SVM Hyper Parameters}\label{svm-hyper-parameters}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  C (Cost)

  Purpose: Regularization parameter controlling the trade-off between
  training accuracy and model generalization.

  Effect:

  Small C: Allows more misclassifications on the training data â†’ wider
  margin â†’ better generalization, less risk of overfitting.

  Large C: Tries to classify all training examples correctly â†’ narrower
  margin â†’ risk of overfitting.

  Intuition: Think of C as how ``strict'' the SVM is about classifying
  the training points correctly.
\item
  Kernel

  Purpose: Transforms data into a higher-dimensional space so that it
  may become linearly separable.

  Common kernels:

  Linear: Use when data is already linearly separable.

  Polynomial: Adds polynomial combinations of features; good for curved
  decision boundaries.

  RBF (Gaussian): Maps data into infinite-dimensional space; popular
  default choice for non-linear problems.

  Sigmoid: Similar to a neural network activation function; less
  commonly used.

  Intuition: The kernel decides the ``shape'' of the decision boundary.
\item
  Gamma (Î³)

  Purpose: Defines how far the influence of a single training point
  reaches in the feature space.

  Effect (RBF or polynomial kernels):

  Low gamma: Each point has broad influence â†’ smoother decision boundary
  â†’ less risk of overfitting.

  High gamma: Each point has very localized influence â†’ more complex
  boundary â†’ higher risk of overfitting.

  Intuition: Think of gamma as how ``far'' each point can reach to
  affect the boundary.
\end{enumerate}

\bookmarksetup{startatroot}

\chapter*{References}\label{references}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{0}{1}
\end{CSLReferences}




\end{document}
