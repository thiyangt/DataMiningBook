[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Mining",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "ch5.html",
    "href": "ch5.html",
    "title": "5¬† Measures of Performance",
    "section": "",
    "text": "5.1 Load libraries\nTask: Build a tree-based model to predict species\nlibrary(palmerpenguins)\nlibrary(rsample)       # for initial_split\nlibrary(dplyr)\nlibrary(caret)\nlibrary(randomForest)\nlibrary(ada)\nlibrary(gbm)\nlibrary(tidyverse)\nlibrary(yardstick) #roc_curve",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Measures of Performance</span>"
    ]
  },
  {
    "objectID": "ch5.html#clean-data",
    "href": "ch5.html#clean-data",
    "title": "5¬† Measures of Performance",
    "section": "5.2 Clean data",
    "text": "5.2 Clean data\n\npenguins_clean &lt;- na.omit(penguins)\npenguins_clean$species &lt;- as.factor(penguins_clean$species)",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Measures of Performance</span>"
    ]
  },
  {
    "objectID": "ch5.html#stratified-train-test-split",
    "href": "ch5.html#stratified-train-test-split",
    "title": "5¬† Measures of Performance",
    "section": "5.3 Stratified train-test split",
    "text": "5.3 Stratified train-test split\n\nset.seed(123)\nsplit &lt;- initial_split(penguins_clean, prop = 0.7, strata = \"species\")\ntrain_data &lt;- training(split)\ntable(train_data$species)\n\n\n   Adelie Chinstrap    Gentoo \n      102        47        83 \n\ntest_data  &lt;- testing(split)\ntable(test_data$species)\n\n\n   Adelie Chinstrap    Gentoo \n       44        21        36",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Measures of Performance</span>"
    ]
  },
  {
    "objectID": "ch5.html#variables",
    "href": "ch5.html#variables",
    "title": "5¬† Measures of Performance",
    "section": "5.4 Variables",
    "text": "5.4 Variables\n\npredictors &lt;- c(\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\", \"sex\")\nresponse &lt;- \"species\"",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Measures of Performance</span>"
    ]
  },
  {
    "objectID": "ch5.html#random-forest-model",
    "href": "ch5.html#random-forest-model",
    "title": "5¬† Measures of Performance",
    "section": "5.5 Random Forest Model",
    "text": "5.5 Random Forest Model\n\nset.seed(123)\nrf_model &lt;- randomForest(\n  as.formula(paste(response, \"~\", paste(predictors, collapse = \"+\"))),\n  data = train_data,\n  importance = TRUE\n)\nrf_model\n\n\nCall:\n randomForest(formula = as.formula(paste(response, \"~\", paste(predictors,      collapse = \"+\"))), data = train_data, importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 3.45%\nConfusion matrix:\n          Adelie Chinstrap Gentoo class.error\nAdelie       100         1      1  0.01960784\nChinstrap      4        42      1  0.10638298\nGentoo         0         1     82  0.01204819\n\nrf_pred &lt;- predict(rf_model, newdata = test_data)\nrf_pred \n\n        1         2         3         4         5         6         7         8 \n   Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie \n        9        10        11        12        13        14        15        16 \n   Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie \n       17        18        19        20        21        22        23        24 \n   Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie \n       25        26        27        28        29        30        31        32 \n   Adelie Chinstrap    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie \n       33        34        35        36        37        38        39        40 \n   Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie \n       41        42        43        44        45        46        47        48 \n   Adelie    Adelie    Adelie    Adelie    Gentoo    Gentoo    Gentoo    Gentoo \n       49        50        51        52        53        54        55        56 \n   Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo \n       57        58        59        60        61        62        63        64 \n   Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo \n       65        66        67        68        69        70        71        72 \n   Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo \n       73        74        75        76        77        78        79        80 \n   Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo \n       81        82        83        84        85        86        87        88 \nChinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap \n       89        90        91        92        93        94        95        96 \nChinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap \n       97        98        99       100       101 \nChinstrap Chinstrap Chinstrap Chinstrap Chinstrap \nLevels: Adelie Chinstrap Gentoo\n\nrf_cm &lt;- confusionMatrix(rf_pred, test_data$species)\nprint(rf_cm)\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  Adelie Chinstrap Gentoo\n  Adelie        43         0      0\n  Chinstrap      1        21      0\n  Gentoo         0         0     36\n\nOverall Statistics\n                                          \n               Accuracy : 0.9901          \n                 95% CI : (0.9461, 0.9997)\n    No Information Rate : 0.4356          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.9846          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Adelie Class: Chinstrap Class: Gentoo\nSensitivity                 0.9773           1.0000        1.0000\nSpecificity                 1.0000           0.9875        1.0000\nPos Pred Value              1.0000           0.9545        1.0000\nNeg Pred Value              0.9828           1.0000        1.0000\nPrevalence                  0.4356           0.2079        0.3564\nDetection Rate              0.4257           0.2079        0.3564\nDetection Prevalence        0.4257           0.2178        0.3564\nBalanced Accuracy           0.9886           0.9938        1.0000",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Measures of Performance</span>"
    ]
  },
  {
    "objectID": "ch5.html#exercise",
    "href": "ch5.html#exercise",
    "title": "5¬† Measures of Performance",
    "section": "5.6 Exercise",
    "text": "5.6 Exercise\nCompute the following measures manually and interpret them.\n\nSensitivity\nSpecificity\nPrevalence\nPositive Prediction Value (PPV)\nNegative Prediction Value (NPV)\nDetection rate\nDetection prevalence\nBalanced accuracy\nPrecision\nRecall\nF1 score",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Measures of Performance</span>"
    ]
  },
  {
    "objectID": "ch5.html#receiver-operating-characteristic-roc-curves",
    "href": "ch5.html#receiver-operating-characteristic-roc-curves",
    "title": "5¬† Measures of Performance",
    "section": "5.7 Receiver Operating Characteristic (ROC) curves",
    "text": "5.7 Receiver Operating Characteristic (ROC) curves\nROC curves are normally for binary classification, but penguins$species has three classes. For multiclass ROC, we compute one-vs-all ROC curves.\nIt‚Äôs a plot of:\n\nTrue Positive Rate (TPR / Sensitivity) on the y-axis\nFalse Positive Rate (FPR = 1 - Specificity) on the x-axis\nIt shows the trade-off between sensitivity and specificity for different decision thresholds of a classifier.\n\n\n5.7.1 Metric Formula\nTrue Positive Rate (Sensitivity)\n\\[TPR = TP / (TP + FN)\\]\nFalse Positive Rate\n\\[FPR = FP / (FP + TN)\\]\nSpecificity\n\\[TN / (TN + FP) = 1 ‚àí FPR\\]\nWhere:\n\\(TP\\) = True Positives\n\\(FP\\) = False Positives\n\\(TN\\) = True Negatives\n\\(FN\\) = False Negatives\n\n# Predict class probabilities\nrf_prob &lt;- predict(rf_model, newdata = test_data, type = \"prob\")\nrf_prob\n\n    Adelie Chinstrap Gentoo\n1    0.978     0.022  0.000\n2    0.924     0.076  0.000\n3    0.990     0.010  0.000\n4    0.976     0.024  0.000\n5    0.972     0.028  0.000\n6    0.994     0.006  0.000\n7    0.994     0.006  0.000\n8    0.964     0.036  0.000\n9    0.994     0.006  0.000\n10   0.984     0.016  0.000\n11   0.718     0.280  0.002\n12   0.998     0.002  0.000\n13   0.998     0.002  0.000\n14   0.994     0.006  0.000\n15   0.952     0.048  0.000\n16   0.966     0.034  0.000\n17   1.000     0.000  0.000\n18   0.996     0.004  0.000\n19   0.966     0.034  0.000\n20   0.992     0.008  0.000\n21   0.894     0.068  0.038\n22   0.984     0.016  0.000\n23   0.990     0.010  0.000\n24   1.000     0.000  0.000\n25   0.974     0.026  0.000\n26   0.262     0.734  0.004\n27   0.802     0.196  0.002\n28   0.864     0.088  0.048\n29   0.978     0.022  0.000\n30   0.934     0.060  0.006\n31   0.980     0.020  0.000\n32   0.750     0.250  0.000\n33   0.986     0.014  0.000\n34   0.998     0.002  0.000\n35   0.982     0.018  0.000\n36   0.968     0.032  0.000\n37   0.988     0.012  0.000\n38   0.998     0.002  0.000\n39   1.000     0.000  0.000\n40   0.980     0.020  0.000\n41   0.986     0.014  0.000\n42   0.990     0.010  0.000\n43   0.958     0.042  0.000\n44   0.898     0.060  0.042\n45   0.000     0.000  1.000\n46   0.000     0.004  0.996\n47   0.000     0.000  1.000\n48   0.000     0.000  1.000\n49   0.002     0.006  0.992\n50   0.006     0.014  0.980\n51   0.000     0.000  1.000\n52   0.000     0.012  0.988\n53   0.000     0.000  1.000\n54   0.000     0.004  0.996\n55   0.016     0.008  0.976\n56   0.000     0.000  1.000\n57   0.000     0.000  1.000\n58   0.014     0.004  0.982\n59   0.016     0.026  0.958\n60   0.000     0.000  1.000\n61   0.002     0.008  0.990\n62   0.004     0.026  0.970\n63   0.002     0.000  0.998\n64   0.000     0.000  1.000\n65   0.000     0.000  1.000\n66   0.000     0.000  1.000\n67   0.000     0.004  0.996\n68   0.000     0.000  1.000\n69   0.000     0.000  1.000\n70   0.000     0.000  1.000\n71   0.000     0.000  1.000\n72   0.000     0.002  0.998\n73   0.000     0.000  1.000\n74   0.000     0.000  1.000\n75   0.002     0.006  0.992\n76   0.012     0.020  0.968\n77   0.000     0.000  1.000\n78   0.000     0.000  1.000\n79   0.000     0.000  1.000\n80   0.000     0.000  1.000\n81   0.018     0.982  0.000\n82   0.032     0.968  0.000\n83   0.040     0.960  0.000\n84   0.022     0.978  0.000\n85   0.162     0.838  0.000\n86   0.052     0.944  0.004\n87   0.038     0.958  0.004\n88   0.070     0.920  0.010\n89   0.030     0.966  0.004\n90   0.204     0.794  0.002\n91   0.030     0.966  0.004\n92   0.036     0.940  0.024\n93   0.026     0.968  0.006\n94   0.334     0.506  0.160\n95   0.022     0.976  0.002\n96   0.250     0.456  0.294\n97   0.004     0.990  0.006\n98   0.030     0.966  0.004\n99   0.034     0.960  0.006\n100  0.202     0.722  0.076\n101  0.328     0.474  0.198\nattr(,\"class\")\n[1] \"matrix\" \"array\"  \"votes\" \n\n\n\n# Combine predicted probabilities with true labels for tidymodels\nrf_results &lt;- test_data %&gt;%\n  select(species) %&gt;%\n  bind_cols(as.data.frame(rf_prob))\nrf_results\n\n# A tibble: 101 √ó 4\n   species Adelie Chinstrap Gentoo\n * &lt;fct&gt;    &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n 1 Adelie   0.978     0.022      0\n 2 Adelie   0.924     0.076      0\n 3 Adelie   0.99      0.01       0\n 4 Adelie   0.976     0.024      0\n 5 Adelie   0.972     0.028      0\n 6 Adelie   0.994     0.006      0\n 7 Adelie   0.994     0.006      0\n 8 Adelie   0.964     0.036      0\n 9 Adelie   0.994     0.006      0\n10 Adelie   0.984     0.016      0\n# ‚Ñπ 91 more rows\n\n\n\n# Compute ROC curves\n# roc_curve() for multiclass expects:\n# truth column and all class probability columns\nroc_data &lt;- rf_results %&gt;%\n  roc_curve(truth = species, Adelie, Chinstrap, Gentoo) \nroc_data\n\n# A tibble: 129 √ó 4\n   .level .threshold specificity sensitivity\n   &lt;chr&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 Adelie   -Inf           0               1\n 2 Adelie      0           0               1\n 3 Adelie      0.002       0.456           1\n 4 Adelie      0.004       0.526           1\n 5 Adelie      0.006       0.561           1\n 6 Adelie      0.012       0.579           1\n 7 Adelie      0.014       0.596           1\n 8 Adelie      0.016       0.614           1\n 9 Adelie      0.018       0.649           1\n10 Adelie      0.022       0.667           1\n# ‚Ñπ 119 more rows\n\n\n\n# Plot ROC curves\nroc_data |&gt; autoplot()  \n\n\n\n\n\n\n\n\n\n# Compute AUC for each class\nauc_data &lt;- rf_results |&gt;\n  roc_auc(truth = species, Adelie, Chinstrap, Gentoo)\nauc_data\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc hand_till      0.999",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Measures of Performance</span>"
    ]
  },
  {
    "objectID": "ch5.html#illustration-of-how-roc-curves-are-plotted",
    "href": "ch5.html#illustration-of-how-roc-curves-are-plotted",
    "title": "5¬† Measures of Performance",
    "section": "5.8 Illustration of How ROC Curves Are Plotted",
    "text": "5.8 Illustration of How ROC Curves Are Plotted\nLet‚Äôs do that for one species (Adelie) vs others, using predicted probabilities from a Random Forest.\n\ntest_data_adelie &lt;- test_data |&gt;\n  mutate(actual = ifelse(species == \"Adelie\", 1, 0))\ntest_data_adelie |&gt; glimpse()\n\nRows: 101\nColumns: 9\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel‚Ä¶\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Biscoe, ‚Ä¶\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 39.3, 36.6, 37.7, 35.9, 38.2, 38.8, 37.9‚Ä¶\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 20.6, 17.8, 18.7, 19.2, 18.1, 17.2, 18.6‚Ä¶\n$ flipper_length_mm &lt;int&gt; 181, 186, 190, 185, 180, 189, 185, 180, 172, 188, 18‚Ä¶\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3650, 3700, 3600, 3800, 3950, 3800, 3150‚Ä¶\n$ sex               &lt;fct&gt; male, female, male, female, male, female, male, male‚Ä¶\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007‚Ä¶\n$ actual            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1‚Ä¶\n\n\nWe can calculate TPR and FPR at different thresholds:\n\nthresholds &lt;- seq(0, 1, by = 0.1)  # thresholds from 0 to 1\nroc_manual &lt;- data.frame(threshold = thresholds, TPR = NA, FPR = NA)\n\nfor (i in seq_along(thresholds)) {\n  thresh &lt;- thresholds[i]\n  \n  # Predicted class based on threshold\n  pred_class &lt;- ifelse(test_data$pred_adelie &gt;= thresh, 1, 0)\n  \n  # Confusion matrix components\n  TP &lt;- sum(pred_class == 1 & test_data$actual == 1)\n  FP &lt;- sum(pred_class == 1 & test_data$actual == 0)\n  FN &lt;- sum(pred_class == 0 & test_data$actual == 1)\n  TN &lt;- sum(pred_class == 0 & test_data$actual == 0)\n  \n  # Compute TPR and FPR\n  roc_manual$TPR[i] &lt;- TP / (TP + FN)\n  roc_manual$FPR[i] &lt;- FP / (FP + TN)\n}\n\nroc_manual\n\n   threshold TPR FPR\n1        0.0 NaN NaN\n2        0.1 NaN NaN\n3        0.2 NaN NaN\n4        0.3 NaN NaN\n5        0.4 NaN NaN\n6        0.5 NaN NaN\n7        0.6 NaN NaN\n8        0.7 NaN NaN\n9        0.8 NaN NaN\n10       0.9 NaN NaN\n11       1.0 NaN NaN\n\n\nThe NaN (Not a Number) appears because some thresholds may produce divisions by zero, typically when (TP + FN) or (FP + TN) is zero. This can happen if the classifier predicts all 0s or all 1s for extreme thresholds (0 or 1).\n\nthresholds &lt;- seq(0, 1, by = 0.1)  # thresholds from 0 to 1\nroc_manual &lt;- data.frame(threshold = thresholds, TPR = NA, FPR = NA)\n\nfor (i in seq_along(thresholds)) {\n  thresh &lt;- thresholds[i]\n  \n  # Predicted class based on threshold\n  pred_class &lt;- ifelse(test_data$pred_adelie &gt;= thresh, 1, 0)\n  \n  # Confusion matrix components\n  TP &lt;- sum(pred_class == 1 & test_data$actual == 1)\n  FP &lt;- sum(pred_class == 1 & test_data$actual == 0)\n  FN &lt;- sum(pred_class == 0 & test_data$actual == 1)\n  TN &lt;- sum(pred_class == 0 & test_data$actual == 0)\n  \n  # Compute TPR safely\n  roc_manual$TPR[i] &lt;- if ((TP + FN) == 0) 0 else TP / (TP + FN)\n  roc_manual$FPR[i] &lt;- if ((FP + TN) == 0) 0 else FP / (FP + TN)\n}\n\nprint(roc_manual)\n\n   threshold TPR FPR\n1        0.0   0   0\n2        0.1   0   0\n3        0.2   0   0\n4        0.3   0   0\n5        0.4   0   0\n6        0.5   0   0\n7        0.6   0   0\n8        0.7   0   0\n9        0.8   0   0\n10       0.9   0   0\n11       1.0   0   0",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Measures of Performance</span>"
    ]
  },
  {
    "objectID": "ch5.html#variable-importance-measures",
    "href": "ch5.html#variable-importance-measures",
    "title": "5¬† Measures of Performance",
    "section": "5.9 Variable Importance Measures",
    "text": "5.9 Variable Importance Measures\nRandom Forest provides two widely used importance metrics:\n(a) Mean Decrease in Gini (Gini Importance)\nMeasures how much a variable reduces node impurity (Gini index) across all trees.\n\nVariables used for highly discriminative splits receive larger importance values.\n\nHigher value ‚Üí more important predictor.\n(b) Mean Decrease in Accuracy (Permutation Importance)\nAfter the model is trained, the values of a variable are randomly permuted.\n\nIf the model accuracy drops significantly, the variable is important.\n\nReflects the predictive power of each variable.\nInterpretation:\nShows which features the random forest relied on most.\n\nimportance(rf_model)\n\n                     Adelie Chinstrap    Gentoo MeanDecreaseAccuracy\nbill_length_mm    95.948710 63.227999 14.989892             85.86134\nbill_depth_mm     20.855953 17.906902 30.126735             35.12314\nflipper_length_mm 18.476543 28.099211 28.799293             34.80012\nbody_mass_g        9.477884 19.466334 15.807238             21.34092\nsex                7.976165  8.382336  6.158169             11.40692\n                  MeanDecreaseGini\nbill_length_mm            57.18023\nbill_depth_mm             28.01945\nflipper_length_mm         44.80369\nbody_mass_g               15.60508\nsex                        1.49749\n\n\n1. Mean Decrease in Accuracy (Permutation Importance)\nThis measures how much the model‚Äôs accuracy drops when the values of each variable are randomly permuted. Higher values ‚Üí more important.\nInterpretation:\nbill_length_mm (85.86) is by far the most important variable.\n‚Üí Shuffling this variable causes a large drop in accuracy.\n‚Üí Species are strongly separated by bill length.\nbill_depth_mm (35.12) and flipper_length_mm (34.80)\n‚Üí Moderately important.\n‚Üí They help the model differentiate species, but less than bill length.\nbody_mass_g (21.34)\n‚Üí Some importance.\n‚Üí Body mass varies between species but overlaps, so it is less discriminative.\nsex (11.41)\n‚Üí Least important.\n‚Üí Sex does not significantly differentiate species\n2. Mean Decrease in Gini (Impurity Importance)\nThis measures how much each variable reduces node impurity in the decision trees. Higher values ‚Üí variable used for strong, clean splits.\nInterpretation:\nbill_length_mm (57.18)\n‚Üí Most useful for creating pure nodes.\n‚Üí Confirms it is the single best predictor of species.\nflipper_length_mm (44.80)\n‚Üí Also very important for clean splits.\nbill_depth_mm (28.02)\n‚Üí Useful but less than the above.\nbody_mass_g (15.61)\n‚Üí Small contribution.\nsex (1.50)\n‚Üí Barely contributes to splitting.\n‚Üí Almost irrelevant for species classification.\n\nvarImpPlot(rf_model,\n           main = \"Random Forest Variable Importance\",\n           pch = 16,\n           col = \"blue\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Measures of Performance</span>"
    ]
  },
  {
    "objectID": "ch5.html#ggplot2-variable-importance-plot-gini-importance",
    "href": "ch5.html#ggplot2-variable-importance-plot-gini-importance",
    "title": "5¬† Measures of Performance",
    "section": "5.10 ggplot2 Variable Importance Plot (Gini Importance)",
    "text": "5.10 ggplot2 Variable Importance Plot (Gini Importance)\n\nlibrary(ggplot2)\n\nimp &lt;- importance(rf_model)\nimp_df &lt;- data.frame(\n  variable = rownames(imp),\n  MeanDecreaseGini = imp[, \"MeanDecreaseGini\"]\n)\n\nggplot(imp_df, aes(x = reorder(variable, MeanDecreaseGini),\n                   y = MeanDecreaseGini)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Random Forest Variable Importance (Mean Decrease Gini)\",\n       x = \"Predictor\",\n       y = \"Importance (Gini)\") +\n  theme_minimal(base_size = 14)",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Measures of Performance</span>"
    ]
  },
  {
    "objectID": "ch5.html#ggplot2-variable-importance-plot-permutation-importance",
    "href": "ch5.html#ggplot2-variable-importance-plot-permutation-importance",
    "title": "5¬† Measures of Performance",
    "section": "5.11 ggplot2 Variable Importance Plot (Permutation Importance)",
    "text": "5.11 ggplot2 Variable Importance Plot (Permutation Importance)\n\nimp2_df &lt;- data.frame(\n  variable = rownames(imp),\n  MeanDecreaseAccuracy = imp[, \"MeanDecreaseAccuracy\"]\n)\n\nggplot(imp2_df, aes(x = reorder(variable, MeanDecreaseAccuracy),\n                    y = MeanDecreaseAccuracy)) +\n  geom_col(fill = \"darkgreen\") +\n  coord_flip() +\n  labs(title = \"Random Forest Variable Importance (Permutation Importance)\",\n       x = \"Predictor\",\n       y = \"Importance (Mean Decrease Accuracy)\") +\n  theme_minimal(base_size = 14)",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Measures of Performance</span>"
    ]
  },
  {
    "objectID": "ch6.html",
    "href": "ch6.html",
    "title": "6¬† k-fold Cross Validation Approaches",
    "section": "",
    "text": "6.1 Load Packages\nlibrary(randomForest)\nlibrary(pROC)\nlibrary(caret)\nlibrary(palmerpenguins)",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>k-fold Cross Validation Approaches</span>"
    ]
  },
  {
    "objectID": "ch6.html#prepare-data",
    "href": "ch6.html#prepare-data",
    "title": "6¬† k-fold Cross Validation Approaches",
    "section": "6.2 Prepare data",
    "text": "6.2 Prepare data\n\npenguins &lt;- penguins |&gt; na.omit()\npenguins$species &lt;- factor(penguins$species)",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>k-fold Cross Validation Approaches</span>"
    ]
  },
  {
    "objectID": "ch6.html#create-k-fold-cross-validation",
    "href": "ch6.html#create-k-fold-cross-validation",
    "title": "6¬† k-fold Cross Validation Approaches",
    "section": "6.3 Create k-fold cross validation",
    "text": "6.3 Create k-fold cross validation\n\nset.seed(123)\nfolds &lt;- createFolds(penguins$species, k = 5)\nfolds\n\n$Fold1\n [1]   8  13  16  20  23  27  36  51  56  57  58  60  63  64  87  90  91  94  96\n[20] 106 110 112 118 124 125 130 134 138 139 151 152 154 156 166 181 182 190 200\n[39] 204 206 211 213 222 234 235 236 238 240 243 244 254 257 263 266 273 277 278\n[58] 284 291 302 309 310 312 313 317 324\n\n$Fold2\n [1]  10  12  14  19  22  30  33  43  46  47  54  61  68  71  72  74  78  82  92\n[20]  97 102 104 107 108 111 119 120 123 132 147 149 158 160 161 169 174 178 180\n[39] 191 192 198 199 201 202 215 219 226 231 237 242 248 250 259 267 268 270 274\n[58] 280 287 300 316 319 320 322 332 333\n\n$Fold3\n [1]   3   4   5  15  17  18  25  29  32  35  37  40  44  45  48  66  70  86  89\n[20]  93  99 105 116 122 129 131 136 140 141 144 157 165 172 177 183 184 186 187\n[39] 194 195 197 205 207 209 210 212 217 218 220 228 229 232 262 276 281 282 285\n[58] 288 289 292 293 298 304 315 323 325 329\n\n$Fold4\n [1]   1   6   9  11  24  26  28  38  39  41  52  62  67  69  79  81  83  84  85\n[20]  88  98 100 109 115 128 135 143 145 146 153 155 159 164 167 168 175 188 193\n[39] 196 208 216 221 223 224 227 230 233 241 247 256 258 261 265 269 271 286 290\n[58] 299 301 303 305 306 307 308 311 321 331\n\n$Fold5\n [1]   2   7  21  31  34  42  49  50  53  55  59  65  73  75  76  77  80  95 101\n[20] 103 113 114 117 121 126 127 133 137 142 148 150 162 163 170 171 173 176 179\n[39] 185 189 203 214 225 239 245 246 249 251 252 253 255 260 264 272 275 279 283\n[58] 294 295 296 297 314 318 326 327 328 330",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>k-fold Cross Validation Approaches</span>"
    ]
  },
  {
    "objectID": "ch6.html#run-random-forest-on-each-fold",
    "href": "ch6.html#run-random-forest-on-each-fold",
    "title": "6¬† k-fold Cross Validation Approaches",
    "section": "6.4 Run random forest on each fold",
    "text": "6.4 Run random forest on each fold\n\nresults &lt;- data.frame(\n  Fold = 1:5,\n  Accuracy = NA,\n  Sensitivity = NA,\n  Specificity = NA,\n  AUC = NA\n)\n\nroc_list &lt;- list()\n\nfor (i in 1:5) {\n  \n  test_idx  &lt;- folds[[i]]\n  train_set &lt;- penguins[-test_idx, ]\n  test_set  &lt;- penguins[test_idx, ]\n  \n  # Fit RF\n  rf_model &lt;- randomForest(\n    species ~ bill_length_mm + bill_depth_mm +\n      flipper_length_mm + body_mass_g + sex,\n    data = train_set,\n    ntree = 500,\n    mtry = 3,\n    importance = TRUE\n  )\n  \n  # Predict class\n  pred_class &lt;- predict(rf_model, test_set)\n  \n  # Predict probabilities\n  pred_prob &lt;- predict(rf_model, test_set, type = \"prob\")\n  \n  # Confusion matrix\n  cm &lt;- confusionMatrix(pred_class, test_set$species)\n  \n  results$Accuracy[i]    &lt;- cm$overall[\"Accuracy\"]\n  results$Sensitivity[i] &lt;- mean(cm$byClass[, \"Sensitivity\"], na.rm = TRUE)\n  results$Specificity[i] &lt;- mean(cm$byClass[, \"Specificity\"], na.rm = TRUE)\n  \n  # Hand‚ÄìTill AUC for MULTICLASS\n  roc_obj &lt;- multiclass.roc(response = test_set$species, predictor = pred_prob)\n  results$AUC[i] &lt;- auc(roc_obj)\n  \n  roc_list[[i]] &lt;- roc_obj\n}\n\nresults\n\n  Fold  Accuracy Sensitivity Specificity       AUC\n1    1 0.9545455   0.9631226   0.9794849 0.9990790\n2    2 0.9848485   0.9885057   0.9937107 0.9986737\n3    3 0.9850746   0.9888889   0.9937107 1.0000000\n4    4 0.9850746   0.9761905   0.9912281 1.0000000\n5    5 0.9701493   0.9523810   0.9824561 0.9958949",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>k-fold Cross Validation Approaches</span>"
    ]
  },
  {
    "objectID": "ch6.html#when-using-random-forest-with-cross-validation-what-is-the-final-model",
    "href": "ch6.html#when-using-random-forest-with-cross-validation-what-is-the-final-model",
    "title": "6¬† k-fold Cross Validation Approaches",
    "section": "6.5 When using Random Forest with Cross-Validation, what is the final model?",
    "text": "6.5 When using Random Forest with Cross-Validation, what is the final model?\nDuring k-fold cross-validation, you train K different random forest models, each on a different subset of the data.\nBut none of those K models is your final model.\nThe purpose of Cross Validation in Random Forest\n‚úîÔ∏è Evaluate multiple hyperparameter combinations\nFor example:\nmtry (number of variables randomly chosen at each split)\nntree (number of trees)\nnodesize\nmaxnodes\n‚úîÔ∏è Measure model performance for each hyperparameter set\nUsing:\nAccuracy\nAUC\nSensitivity / Specificity\nKappa\nLog-loss (if needed)\n‚úîÔ∏è Choose the best-performing hyperparameter combination\n(Usually the one giving highest CV accuracy or AUC.)\nüéØ Then what?\nAfter identifying the best hyperparameters\nüëâ You retrain one final random forest model on the entire dataset using ONLY the best hyperparameters.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>k-fold Cross Validation Approaches</span>"
    ]
  },
  {
    "objectID": "ch6.html#example",
    "href": "ch6.html#example",
    "title": "6¬† k-fold Cross Validation Approaches",
    "section": "6.6 Example",
    "text": "6.6 Example\n\n6.6.1 Hyperparameter Grid\n\ngrid &lt;- expand.grid(\n  mtry  = c(2, 3, 4),\n  ntree = c(300, 500, 800)\n)\ngrid\n\n  mtry ntree\n1    2   300\n2    3   300\n3    4   300\n4    2   500\n5    3   500\n6    4   500\n7    2   800\n8    3   800\n9    4   800\n\n\n\n\n6.6.2 Set up 5-fold cross-validation\n\nset.seed(123)\nfolds &lt;- createFolds(penguins$species, k = 5)\n\n\n\n6.6.3 Run cross-validation for each hyperparameter combination\n\ncv_results &lt;- data.frame(\n  mtry = grid$mtry,\n  ntree = grid$ntree,\n  Accuracy = NA\n)\n\nfor (i in 1:nrow(grid)) {\n  \n  m &lt;- grid$mtry[i]\n  t &lt;- grid$ntree[i]\n  \n  acc_vec &lt;- c()\n  \n  for (k in 1:5) {\n    \n    test_idx  &lt;- folds[[k]]\n    train_set &lt;- penguins[-test_idx, ]\n    test_set  &lt;- penguins[test_idx, ]\n    \n    # Fit model\n    rf_model &lt;- randomForest(\n      species ~ bill_length_mm + bill_depth_mm +\n        flipper_length_mm + body_mass_g + sex,\n      data = train_set,\n      mtry = m,\n      ntree = t\n    )\n    \n    # Predict\n    pred &lt;- predict(rf_model, test_set)\n    \n    # Accuracy for this fold\n    acc &lt;- mean(pred == test_set$species)\n    acc_vec &lt;- c(acc_vec, acc)\n  }\n  \n  # Store mean CV accuracy\n  cv_results$Accuracy[i] &lt;- mean(acc_vec)\n}\n\ncv_results\n\n  mtry ntree  Accuracy\n1    2   300 0.9819539\n2    3   300 0.9819539\n3    4   300 0.9789236\n4    2   500 0.9819539\n5    3   500 0.9819539\n6    4   500 0.9789236\n7    2   800 0.9819539\n8    3   800 0.9789236\n9    4   800 0.9789236\n\n\n\n\n6.6.4 Select the best hyperparameters\n\nbest_row &lt;- cv_results[which.max(cv_results$Accuracy), ]\nbest_row\n\n  mtry ntree  Accuracy\n1    2   300 0.9819539",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>k-fold Cross Validation Approaches</span>"
    ]
  },
  {
    "objectID": "ch7.html",
    "href": "ch7.html",
    "title": "7¬† Extreme Gradient Boosting (XGBoost)",
    "section": "",
    "text": "7.1 Without cross validation\nExtreme Gradient Boosting (XGBoost) is a machine learning algorithm that uses gradient boosted decision trees. It is part of a new generation of algorithms that are designed to be highly accurate.\nXGBoost improves accuracy mainly by reducing overfitting during training. This is achieved through its objective function, which has two parts:\nBy combining these, XGBoost builds a model that is both accurate and generalizes well to new data.\n# Install packages if not already installed\n# install.packages(\"xgboost\")\n# install.packages(\"caret\")\n# install.packages(\"Matrix\")\n\nlibrary(xgboost)\nlibrary(caret)\nlibrary(Matrix)\n\n# Example dataset: Iris (binary classification: setosa vs others)\ndata(iris)\niris$label &lt;- ifelse(iris$Species == \"setosa\", 1, 0)  # binary label\niris$Species &lt;- NULL  # remove original factor\n\n# Split data into training and test sets\nset.seed(123)\ntrain_index &lt;- createDataPartition(iris$label, p = 0.8, list = FALSE)\ntrain_data &lt;- iris[train_index, ]\ntest_data &lt;- iris[-train_index, ]\n\n# Convert data to matrix form\ntrain_matrix &lt;- as.matrix(train_data[, -ncol(train_data)])\ntrain_label &lt;- train_data$label\ntest_matrix &lt;- as.matrix(test_data[, -ncol(test_data)])\ntest_label &lt;- test_data$label\n\n# Create DMatrix objects (XGBoost format)\ndtrain &lt;- xgb.DMatrix(data = train_matrix, label = train_label)\ndtest &lt;- xgb.DMatrix(data = test_matrix, label = test_label)\n\n# Set parameters\nparams &lt;- list(\n  booster = \"gbtree\",\n  objective = \"binary:logistic\",\n  eval_metric = \"logloss\",  # or \"error\"\n  eta = 0.1,  # learning rate\n  max_depth = 3,\n  min_child_weight = 1,\n  subsample = 0.8,\n  colsample_bytree = 0.8\n)\n\n# Train the model\nset.seed(123)\nxgb_model &lt;- xgb.train(\n  params = params,\n  data = dtrain,\n  nrounds = 100,\n  watchlist = list(train = dtrain, test = dtest),\n  early_stopping_rounds = 10,\n  print_every_n = 10\n)\n\nMultiple eval metrics are present. Will use test_logloss for early stopping.\nWill train until test_logloss hasn't improved in 10 rounds.\n\n[1] train-logloss:0.552090  test-logloss:0.552090 \n[11]    train-logloss:0.179963  test-logloss:0.179392 \n[21]    train-logloss:0.074191  test-logloss:0.073715 \n[31]    train-logloss:0.036053  test-logloss:0.035705 \n[41]    train-logloss:0.021509  test-logloss:0.021298 \n[51]    train-logloss:0.019402  test-logloss:0.019212 \nStopping. Best iteration:\n[56]    train-logloss:0.019402  test-logloss:0.019212\n\n[56]    train-logloss:0.019402  test-logloss:0.019212 \n\n# Make predictions\npred_prob &lt;- predict(xgb_model, dtest)\npred_label &lt;- ifelse(pred_prob &gt; 0.5, 1, 0)\n\n# Evaluate\nconfusionMatrix(factor(pred_label), factor(test_label))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 20  0\n         1  0 10\n                                     \n               Accuracy : 1          \n                 95% CI : (0.8843, 1)\n    No Information Rate : 0.6667     \n    P-Value [Acc &gt; NIR] : 5.215e-06  \n                                     \n                  Kappa : 1          \n                                     \n Mcnemar's Test P-Value : NA         \n                                     \n            Sensitivity : 1.0000     \n            Specificity : 1.0000     \n         Pos Pred Value : 1.0000     \n         Neg Pred Value : 1.0000     \n             Prevalence : 0.6667     \n         Detection Rate : 0.6667     \n   Detection Prevalence : 0.6667     \n      Balanced Accuracy : 1.0000     \n                                     \n       'Positive' Class : 0",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Extreme Gradient Boosting (XGBoost)</span>"
    ]
  },
  {
    "objectID": "ch7.html#with-cross-validation",
    "href": "ch7.html#with-cross-validation",
    "title": "7¬† Extreme Gradient Boosting (XGBoost)",
    "section": "7.2 With cross validation",
    "text": "7.2 With cross validation",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Extreme Gradient Boosting (XGBoost)</span>"
    ]
  }
]